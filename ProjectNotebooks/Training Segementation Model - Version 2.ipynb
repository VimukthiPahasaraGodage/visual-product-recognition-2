{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1vlYwayHb1JPAsxgub7i74tOsa3plG0PI","timestamp":1698842410130}],"gpuType":"V100","authorship_tag":"ABX9TyPATnCS1owfwIyPr61J8sLP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import os\n","HOME = os.getcwd()\n","print(\"HOME:\", HOME)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AP2T_Z6HCMaU","executionInfo":{"status":"ok","timestamp":1698864364583,"user_tz":-330,"elapsed":7,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"17424730-2c78-49c5-a2b1-0e22150799e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["HOME: /content\n"]}]},{"cell_type":"code","source":["import locale\n","def getpreferredencoding(do_setlocale = True):\n","    return \"UTF-8\"\n","locale.getpreferredencoding = getpreferredencoding"],"metadata":{"id":"zswLV_X4hLAJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","!pip install open_clip_torch"],"metadata":{"id":"BYSjl4IAVa9t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd {HOME}\n","!git clone https://github.com/IDEA-Research/GroundingDINO.git\n","%cd {HOME}/GroundingDINO\n","!git checkout -q 57535c5a79791cb76e36fdb64975271354f10251\n","!pip install -q -e ."],"metadata":{"id":"qjp55FnCCSQb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698864401359,"user_tz":-330,"elapsed":35345,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"8debc3f0-22d4-459c-f5f7-6fc567b79037"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","fatal: destination path 'GroundingDINO' already exists and is not an empty directory.\n","/content/GroundingDINO\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["%cd {HOME}\n","\n","import sys\n","!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'"],"metadata":{"id":"Ua7yP5tPCViU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698864410764,"user_tz":-330,"elapsed":9464,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"859c64fa-d45a-4915-da65-1400cac0fec3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","Collecting git+https://github.com/facebookresearch/segment-anything.git\n","  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-yund96p7\n","  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-yund96p7\n","  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["!pip uninstall -y supervision\n","!pip install -q supervision==0.6.0\n","\n","import supervision as sv\n","print(sv.__version__)"],"metadata":{"id":"Fsd2W1OaCbxE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698864417712,"user_tz":-330,"elapsed":6957,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"c68a2043-c3b9-4ec8-ea40-fd3652f7400a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: supervision 0.4.0\n","Uninstalling supervision-0.4.0:\n","  Successfully uninstalled supervision-0.4.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","groundingdino 0.1.0 requires supervision==0.4.0, but you have supervision 0.6.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m0.6.0\n"]}]},{"cell_type":"code","source":["import os\n","\n","GROUNDING_DINO_CONFIG_PATH = os.path.join(HOME, \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n","print(GROUNDING_DINO_CONFIG_PATH, \"; exist:\", os.path.isfile(GROUNDING_DINO_CONFIG_PATH))"],"metadata":{"id":"2Cr3XkRqCfsB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698864417712,"user_tz":-330,"elapsed":124,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"fe339053-eccf-4c5e-f052-6b78a7931584"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py ; exist: True\n"]}]},{"cell_type":"code","source":["%cd {HOME}\n","!mkdir -p {HOME}/weights\n","%cd {HOME}/weights\n","\n","!wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth"],"metadata":{"id":"IjmDEZJ1Cl5j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698864423362,"user_tz":-330,"elapsed":5770,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"8d93a71c-b0c3-473b-a087-ef15d4431f9f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","/content/weights\n"]}]},{"cell_type":"code","source":["GROUNDING_DINO_CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"groundingdino_swint_ogc.pth\")\n","print(GROUNDING_DINO_CHECKPOINT_PATH, \"; exist:\", os.path.isfile(GROUNDING_DINO_CHECKPOINT_PATH))"],"metadata":{"id":"vO8pxHUICpUr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698864423362,"user_tz":-330,"elapsed":109,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"f804f394-5ae3-4baf-cf28-af912d05486a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/weights/groundingdino_swint_ogc.pth ; exist: True\n"]}]},{"cell_type":"code","source":["%cd {HOME}\n","!mkdir -p {HOME}/weights\n","%cd {HOME}/weights\n","\n","!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"],"metadata":{"id":"95JKrV16CuIT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698864434667,"user_tz":-330,"elapsed":11409,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"d5ad0507-43c7-4d93-b955-bfc36be2aa5f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","/content/weights\n"]}]},{"cell_type":"code","source":["SAM_CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n","print(SAM_CHECKPOINT_PATH, \"; exist:\", os.path.isfile(SAM_CHECKPOINT_PATH))"],"metadata":{"id":"PotafNnbCxJT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698864434667,"user_tz":-330,"elapsed":33,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"ff276196-17fb-48ee-8d93-e76265154f0a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/weights/sam_vit_h_4b8939.pth ; exist: True\n"]}]},{"cell_type":"code","source":["%cd {HOME}/GroundingDINO\n","\n","from groundingdino.util.inference import Model\n","\n","grounding_dino_model = Model(model_config_path=GROUNDING_DINO_CONFIG_PATH, model_checkpoint_path=GROUNDING_DINO_CHECKPOINT_PATH)"],"metadata":{"id":"Q6IzhGYQC3Zq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698864460070,"user_tz":-330,"elapsed":25432,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"81d2fc9f-c06a-41c6-f59f-e059657b479c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/GroundingDINO\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"]},{"output_type":"stream","name":"stdout","text":["final text_encoder_type: bert-base-uncased\n"]}]},{"cell_type":"code","source":["from segment_anything import sam_model_registry, SamPredictor\n","\n","SAM_ENCODER_VERSION = \"vit_h\"\n","\n","sam = sam_model_registry[SAM_ENCODER_VERSION](checkpoint=SAM_CHECKPOINT_PATH).to('cuda')\n","sam_predictor = SamPredictor(sam)"],"metadata":{"id":"ZxXeHHdNC-uH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader\n","import torchvision.transforms as T\n","from torchvision.io import read_image\n","from torch.utils.data import Dataset\n","import pandas as pd\n","import torchvision.transforms as T\n","import torchvision.transforms.functional as F\n","import pandas as pd\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n","import open_clip\n","from typing import List\n","import math\n","import cv2\n","import copy\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import torch.nn as nn\n","from pathlib import Path\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"TZz7jc1JxTHa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"XgB1ieHefVre","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698864484193,"user_tz":-330,"elapsed":2987,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"a6229921-04b3-410d-f185-4db74218ea66"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["%%capture\n","!unzip /content/gdrive/MyDrive/test-archive.zip -d /content/testing-dataset"],"metadata":{"id":"otSHPS5lfYrd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mv \"/content/testing-dataset/development_test_data/gallery\" \"/content/\"\n","!mv \"/content/testing-dataset/development_test_data/queries\" \"/content/\""],"metadata":{"id":"QCJkTLHAfasf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698864492778,"user_tz":-330,"elapsed":143,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"59167a6c-0e62-42ed-ccfa-defb97952509"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mv: cannot move '/content/testing-dataset/development_test_data/gallery' to '/content/gallery': Directory not empty\n","mv: cannot move '/content/testing-dataset/development_test_data/queries' to '/content/queries': Directory not empty\n"]}]},{"cell_type":"code","source":["!mkdir \"/content/original_datasets\"\n","!mv \"/content/testing-dataset/development_test_data/gallery.csv\" \"/content/original_datasets/\"\n","!mv \"/content/testing-dataset/development_test_data/queries.csv\" \"/content/original_datasets/\""],"metadata":{"id":"Y_UCsHzpfdCV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698864493332,"user_tz":-330,"elapsed":689,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"2f747848-7c54-4cb7-ad45-dc847830f9d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘/content/original_datasets’: File exists\n"]}]},{"cell_type":"code","source":["!rm -rf \"/content/testing-dataset\""],"metadata":{"id":"q_lvc1g6ffn2"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L1IvyNJJa8ak","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698864494088,"user_tz":-330,"elapsed":781,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"fddf26cc-f27b-4144-c346-3721006b9d8a"},"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘/content/image_to_text_results/’: File exists\n"]}],"source":["!mkdir '/content/image_to_text_results/'\n","!cp '/content/gdrive/MyDrive/prompt_What-is-the-product-category_all_version.csv' '/content/image_to_text_results/'\n","!cp '/content/gdrive/MyDrive/prompt_What-is-the-product-category_gallery_version.csv' '/content/image_to_text_results/'\n","!cp '/content/gdrive/MyDrive/prompt_What-is-the-product-category.csv' '/content/image_to_text_results/'"]},{"cell_type":"code","source":["%%capture\n","!unzip '/content/gdrive/MyDrive/gallery_tensors_blip_detection_bbox_crop_bt_0.4_tt_0.25.zip' -d '/'\n","!unzip '/content/gdrive/MyDrive/query_tensors_blip_detection_bbox_crop_bt_0.4_tt_0.25.zip' -d '/'"],"metadata":{"id":"GdcliZn5yyg9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mkdir '/content/pretrained'\n","!cp '/content/gdrive/MyDrive/visual-product-recognition-pre-trained/model1.pt' '/content/pretrained/'\n","!cp '/content/gdrive/MyDrive/visual-product-recognition-pre-trained/model2.pt' '/content/pretrained/'\n","!cp '/content/gdrive/MyDrive/visual-product-recognition-pre-trained/model3.pt' '/content/pretrained/'\n","!cp '/content/gdrive/MyDrive/visual-product-recognition-pre-trained/model4.pt' '/content/pretrained/'"],"metadata":{"id":"Ke6eQEe-Vvi3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698863505462,"user_tz":-330,"elapsed":60109,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"54706c87-594c-449d-8580-7860b79d4502"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘/content/pretrained’: File exists\n"]}]},{"cell_type":"code","source":["masked_similarity = pd.read_csv('/content/gallery_tensors_blip_detection_bbox_crop_bt_0.4_tt_0.25/gallery_tensors_blip_detection_bbox_crop_bt_0.4_tt_0.25.csv', low_memory=False)\n","masked_similarity.head()"],"metadata":{"id":"A81prgPcy-e5","colab":{"base_uri":"https://localhost:8080/","height":399},"executionInfo":{"status":"ok","timestamp":1698853556147,"user_tz":-330,"elapsed":60,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"e5e39136-86f0-446f-e035-84f30328f2ea"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   index                                  image_tensor_path  \\\n","0      0  gallery_tensors_blip_detection_bbox_crop_bt_0....   \n","1      1  gallery_tensors_blip_detection_bbox_crop_bt_0....   \n","2      2  gallery_tensors_blip_detection_bbox_crop_bt_0....   \n","3      3  gallery_tensors_blip_detection_bbox_crop_bt_0....   \n","4      4  gallery_tensors_blip_detection_bbox_crop_bt_0....   \n","\n","                            masked_image_tensor_path  \\\n","0  gallery_tensors_blip_detection_bbox_crop_bt_0....   \n","1  gallery_tensors_blip_detection_bbox_crop_bt_0....   \n","2  gallery_tensors_blip_detection_bbox_crop_bt_0....   \n","3  gallery_tensors_blip_detection_bbox_crop_bt_0....   \n","4  gallery_tensors_blip_detection_bbox_crop_bt_0....   \n","\n","                    inverse_masked_image_tensor_path  label  \\\n","0  gallery_tensors_blip_detection_bbox_crop_bt_0....   5019   \n","1  gallery_tensors_blip_detection_bbox_crop_bt_0....   5008   \n","2  gallery_tensors_blip_detection_bbox_crop_bt_0....      9   \n","3  gallery_tensors_blip_detection_bbox_crop_bt_0....   3589   \n","4  gallery_tensors_blip_detection_bbox_crop_bt_0....   3656   \n","\n","                                         image_path  masked_similarity  \\\n","0      gallery/ambitious-tough-teal-from-asgard.jpg          12.038039   \n","1         gallery/fine-shrewd-oarfish-of-genius.jpg           5.544218   \n","2       gallery/bold-nickel-gecko-of-reputation.jpg           1.164761   \n","3  gallery/marvellous-uber-boobook-of-lightning.jpg           4.094361   \n","4     gallery/bouncy-economic-agama-of-honeydew.jpg           2.484441   \n","\n","   inverse_masked_similarity  \n","0                  18.042643  \n","1                  18.558239  \n","2                  14.268379  \n","3                  12.754694  \n","4                  15.336771  "],"text/html":["\n","  <div id=\"df-175da455-b85b-4ae1-975f-80b9e92e898c\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>image_tensor_path</th>\n","      <th>masked_image_tensor_path</th>\n","      <th>inverse_masked_image_tensor_path</th>\n","      <th>label</th>\n","      <th>image_path</th>\n","      <th>masked_similarity</th>\n","      <th>inverse_masked_similarity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>gallery_tensors_blip_detection_bbox_crop_bt_0....</td>\n","      <td>gallery_tensors_blip_detection_bbox_crop_bt_0....</td>\n","      <td>gallery_tensors_blip_detection_bbox_crop_bt_0....</td>\n","      <td>5019</td>\n","      <td>gallery/ambitious-tough-teal-from-asgard.jpg</td>\n","      <td>12.038039</td>\n","      <td>18.042643</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>gallery_tensors_blip_detection_bbox_crop_bt_0....</td>\n","      <td>gallery_tensors_blip_detection_bbox_crop_bt_0....</td>\n","      <td>gallery_tensors_blip_detection_bbox_crop_bt_0....</td>\n","      <td>5008</td>\n","      <td>gallery/fine-shrewd-oarfish-of-genius.jpg</td>\n","      <td>5.544218</td>\n","      <td>18.558239</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>gallery_tensors_blip_detection_bbox_crop_bt_0....</td>\n","      <td>gallery_tensors_blip_detection_bbox_crop_bt_0....</td>\n","      <td>gallery_tensors_blip_detection_bbox_crop_bt_0....</td>\n","      <td>9</td>\n","      <td>gallery/bold-nickel-gecko-of-reputation.jpg</td>\n","      <td>1.164761</td>\n","      <td>14.268379</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>gallery_tensors_blip_detection_bbox_crop_bt_0....</td>\n","      <td>gallery_tensors_blip_detection_bbox_crop_bt_0....</td>\n","      <td>gallery_tensors_blip_detection_bbox_crop_bt_0....</td>\n","      <td>3589</td>\n","      <td>gallery/marvellous-uber-boobook-of-lightning.jpg</td>\n","      <td>4.094361</td>\n","      <td>12.754694</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>gallery_tensors_blip_detection_bbox_crop_bt_0....</td>\n","      <td>gallery_tensors_blip_detection_bbox_crop_bt_0....</td>\n","      <td>gallery_tensors_blip_detection_bbox_crop_bt_0....</td>\n","      <td>3656</td>\n","      <td>gallery/bouncy-economic-agama-of-honeydew.jpg</td>\n","      <td>2.484441</td>\n","      <td>15.336771</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-175da455-b85b-4ae1-975f-80b9e92e898c')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-175da455-b85b-4ae1-975f-80b9e92e898c button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-175da455-b85b-4ae1-975f-80b9e92e898c');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-7d10e711-ff80-4fc1-b905-0cee66edfaea\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7d10e711-ff80-4fc1-b905-0cee66edfaea')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-7d10e711-ff80-4fc1-b905-0cee66edfaea button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["detection_category = pd.read_csv('/content/image_to_text_results/prompt_What-is-the-product-category_all_version.csv', low_memory=False)\n","detection_category.head()"],"metadata":{"id":"Fyq83-q8vVh1","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1698853556147,"user_tz":-330,"elapsed":56,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"a42894d1-5807-4db0-b9ef-9e01b181c782"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   index  label    category  count\n","0      0   5019       shoes      7\n","1      1   5008       shoes      8\n","2      2      9  basketball      5\n","3      3   3589  sunglasses      3\n","4      4   3656  sunglasses      4"],"text/html":["\n","  <div id=\"df-633f0f3e-bf80-42a9-8e29-89030d571912\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>label</th>\n","      <th>category</th>\n","      <th>count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>5019</td>\n","      <td>shoes</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>5008</td>\n","      <td>shoes</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>9</td>\n","      <td>basketball</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>3589</td>\n","      <td>sunglasses</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>3656</td>\n","      <td>sunglasses</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-633f0f3e-bf80-42a9-8e29-89030d571912')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-633f0f3e-bf80-42a9-8e29-89030d571912 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-633f0f3e-bf80-42a9-8e29-89030d571912');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-663854e8-81c0-4de8-a01e-b43eb949d63c\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-663854e8-81c0-4de8-a01e-b43eb949d63c')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-663854e8-81c0-4de8-a01e-b43eb949d63c button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["gallery = pd.read_csv(\"/content/original_datasets/gallery.csv\", low_memory=False)\n","gallery.head()"],"metadata":{"id":"XDiLxaWtvPbq","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1698853556148,"user_tz":-330,"elapsed":55,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"5d5c48c6-4a2b-479d-89ba-b346d8d69d0f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   seller_img_id                                          img_path  product_id\n","0              0      gallery/ambitious-tough-teal-from-asgard.jpg        5019\n","1              1         gallery/fine-shrewd-oarfish-of-genius.jpg        5008\n","2              2       gallery/bold-nickel-gecko-of-reputation.jpg           9\n","3              3  gallery/marvellous-uber-boobook-of-lightning.jpg        3589\n","4              4     gallery/bouncy-economic-agama-of-honeydew.jpg        3656"],"text/html":["\n","  <div id=\"df-4166ccc3-954c-44d3-adf6-d2df71a72f7e\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>seller_img_id</th>\n","      <th>img_path</th>\n","      <th>product_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>gallery/ambitious-tough-teal-from-asgard.jpg</td>\n","      <td>5019</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>gallery/fine-shrewd-oarfish-of-genius.jpg</td>\n","      <td>5008</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>gallery/bold-nickel-gecko-of-reputation.jpg</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>gallery/marvellous-uber-boobook-of-lightning.jpg</td>\n","      <td>3589</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>gallery/bouncy-economic-agama-of-honeydew.jpg</td>\n","      <td>3656</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4166ccc3-954c-44d3-adf6-d2df71a72f7e')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-4166ccc3-954c-44d3-adf6-d2df71a72f7e button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-4166ccc3-954c-44d3-adf6-d2df71a72f7e');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-4d110007-4095-4d24-b637-9d1fccce117f\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4d110007-4095-4d24-b637-9d1fccce117f')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-4d110007-4095-4d24-b637-9d1fccce117f button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["def get_mask_or_inverse_mask_to_use(img_path):\n","  ms = (masked_similarity.loc[masked_similarity['image_path'] == img_path, 'masked_similarity'].values)[0]\n","  ims = (masked_similarity.loc[masked_similarity['image_path'] == img_path, 'inverse_masked_similarity'].values)[0]\n","  if ms <= ims:\n","    return 1\n","  else:\n","    return 0\n","\n","img_df = gallery\n","df = pd.DataFrame(columns=['img1', 'img2', 'label', 'img1_masked', 'img2_masked', 'img1_category', 'img2_category'])\n","product_ids = gallery['product_id'].unique()\n","row_count = 0\n","id_count = {1:0, 2:0, 3:0, 4:0}\n","for product_id in product_ids:\n","  df1 = img_df.loc[img_df['product_id'] == product_id]\n","  img_category = (detection_category.loc[detection_category['label'] == product_id, 'category'].values)[0]\n","  img_path_index = df1.columns.get_loc('img_path')\n","  if len(df1) == 2:\n","    row_count += 1\n","    id_count[2] += 1\n","    img1 = df1.iloc[0, img_path_index]\n","    img2 = df1.iloc[1, img_path_index]\n","    df.loc[len(df)] = [img1, img2, 1, get_mask_or_inverse_mask_to_use(img1), get_mask_or_inverse_mask_to_use(img2), img_category, img_category]\n","  elif len(df1) == 3:\n","    img1 = df1.iloc[0, img_path_index]\n","    img2 = df1.iloc[1, img_path_index]\n","    img3 = df1.iloc[2, img_path_index]\n","    row_count += 2\n","    id_count[3] += 1\n","    df.loc[len(df)] = [img1, img2, 1, get_mask_or_inverse_mask_to_use(img1), get_mask_or_inverse_mask_to_use(img2), img_category, img_category]\n","    df.loc[len(df)] = [img2, img3, 1, get_mask_or_inverse_mask_to_use(img2), get_mask_or_inverse_mask_to_use(img3), img_category, img_category]\n","  elif len(df1) > 3:\n","    id_count[4] += 1\n","    img1 = df1.iloc[0, img_path_index]\n","    img2 = df1.iloc[1, img_path_index]\n","    img3 = df1.iloc[2, img_path_index]\n","    img4 = df1.iloc[3, img_path_index]\n","    row_count += 2\n","    df.loc[len(df)] = [img1, img2, 1, get_mask_or_inverse_mask_to_use(img1), get_mask_or_inverse_mask_to_use(img2), img_category, img_category]\n","    df.loc[len(df)] = [img3, img4, 1, get_mask_or_inverse_mask_to_use(img3), get_mask_or_inverse_mask_to_use(img4), img_category, img_category]\n","  else:\n","    id_count[1] += 1\n","  df2 = img_df.loc[img_df['product_id'] != product_id]\n","  if len(df1) < 4:\n","    img1 = df1.iloc[0, img_path_index]\n","    count = 0\n","    for index, row in df2.iterrows():\n","      if count >= 4:\n","        break\n","      if ((df['img1'] == img1) & (df['img2'] == row['img_path'])).any() or ((df['img1'] == row['img_path']) & (df['img2'] == img1)).any():\n","        continue\n","      else:\n","        img2 = row['img_path']\n","        row_count += 1\n","        img2_category = (detection_category.loc[detection_category['label'] == row['product_id'], 'category'].values)[0]\n","        df.loc[len(df)] = [img1, img2, 0, get_mask_or_inverse_mask_to_use(img1), get_mask_or_inverse_mask_to_use(img2), img_category, img2_category]\n","        count += 1\n","  else:\n","    img1 = df1.iloc[0, img_path_index]\n","    img2 = df1.iloc[1, img_path_index]\n","    count = 0\n","    for index, row in df2.iterrows():\n","      if count >= 4:\n","        break\n","      if count >= 2:\n","        img1 = img2\n","      if ((df['img1'] == img1) & (df['img2'] == row['img_path'])).any() or ((df['img1'] == row['img_path']) & (df['img2'] == img1)).any():\n","        continue\n","      else:\n","        img3 = row['img_path']\n","        row_count += 1\n","        img2_category = (detection_category.loc[detection_category['label'] == row['product_id'], 'category'].values)[0]\n","        df.loc[len(df)] = [img1, img3, 0, get_mask_or_inverse_mask_to_use(img1), get_mask_or_inverse_mask_to_use(img3), img_category, img2_category]\n","        count += 1\n","\n","df.to_csv('/content/train_validation_dataset.csv')\n","print('row_count: ', row_count)\n","print(len(df.loc[df['label'] == 1]))\n","print(id_count)"],"metadata":{"id":"XmuGH_Syxuxr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698853571338,"user_tz":-330,"elapsed":15242,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"bf1f285a-c8bb-4464-8d71-c525d651d422"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["row_count:  2871\n","435\n","{1: 284, 2: 215, 3: 90, 4: 20}\n"]}]},{"cell_type":"code","source":["print('shape: ', df.shape)\n","display(df.head())"],"metadata":{"id":"fKdw8MyAAVqU","colab":{"base_uri":"https://localhost:8080/","height":311},"executionInfo":{"status":"ok","timestamp":1698853571339,"user_tz":-330,"elapsed":130,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"20ab0c16-ba0e-486e-a3a6-858cb16167b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["shape:  (2871, 7)\n"]},{"output_type":"display_data","data":{"text/plain":["                                           img1  \\\n","0  gallery/ambitious-tough-teal-from-asgard.jpg   \n","1       gallery/busy-aspiring-oryx-of-peace.jpg   \n","2  gallery/ambitious-tough-teal-from-asgard.jpg   \n","3  gallery/ambitious-tough-teal-from-asgard.jpg   \n","4  gallery/ambitious-tough-teal-from-asgard.jpg   \n","\n","                                                img2  label  img1_masked  \\\n","0            gallery/busy-aspiring-oryx-of-peace.jpg      1            1   \n","1  gallery/vigorous-upbeat-barracuda-of-strength.jpg      1            1   \n","2          gallery/fine-shrewd-oarfish-of-genius.jpg      0            1   \n","3        gallery/bold-nickel-gecko-of-reputation.jpg      0            1   \n","4   gallery/marvellous-uber-boobook-of-lightning.jpg      0            1   \n","\n","   img2_masked img1_category img2_category  \n","0            1         shoes         shoes  \n","1            1         shoes         shoes  \n","2            1         shoes         shoes  \n","3            1         shoes    basketball  \n","4            1         shoes    sunglasses  "],"text/html":["\n","  <div id=\"df-27fcd9f2-297b-4644-8e19-b8fdd1ffd5db\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>img1</th>\n","      <th>img2</th>\n","      <th>label</th>\n","      <th>img1_masked</th>\n","      <th>img2_masked</th>\n","      <th>img1_category</th>\n","      <th>img2_category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>gallery/ambitious-tough-teal-from-asgard.jpg</td>\n","      <td>gallery/busy-aspiring-oryx-of-peace.jpg</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>shoes</td>\n","      <td>shoes</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>gallery/busy-aspiring-oryx-of-peace.jpg</td>\n","      <td>gallery/vigorous-upbeat-barracuda-of-strength.jpg</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>shoes</td>\n","      <td>shoes</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>gallery/ambitious-tough-teal-from-asgard.jpg</td>\n","      <td>gallery/fine-shrewd-oarfish-of-genius.jpg</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>shoes</td>\n","      <td>shoes</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>gallery/ambitious-tough-teal-from-asgard.jpg</td>\n","      <td>gallery/bold-nickel-gecko-of-reputation.jpg</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>shoes</td>\n","      <td>basketball</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>gallery/ambitious-tough-teal-from-asgard.jpg</td>\n","      <td>gallery/marvellous-uber-boobook-of-lightning.jpg</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>shoes</td>\n","      <td>sunglasses</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-27fcd9f2-297b-4644-8e19-b8fdd1ffd5db')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-27fcd9f2-297b-4644-8e19-b8fdd1ffd5db button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-27fcd9f2-297b-4644-8e19-b8fdd1ffd5db');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-4b38a4ee-5d3b-4423-bd10-c6853c9c3eac\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4b38a4ee-5d3b-4423-bd10-c6853c9c3eac')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-4b38a4ee-5d3b-4423-bd10-c6853c9c3eac button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{}}]},{"cell_type":"code","source":["df1 = df.loc[df['label'] == 1]\n","df2 = df.loc[df['label'] == 0]\n","\n","msk1 = np.random.rand(len(df1)) < 0.8\n","train1 = df1[msk1]\n","validation1 = df1[~msk1]\n","\n","msk2 = np.random.rand(len(df2)) < 0.8\n","train2 = df2[msk2]\n","validation2 = df2[~msk2]\n","\n","train = pd.concat([train1, train2], axis=0)\n","validation = pd.concat([validation1, validation2], axis=0)\n","\n","train.reset_index(inplace=True)\n","train.drop(['index'], inplace=True, axis=1)\n","train.to_csv('/content/train_dataset_for_segmentation_model.csv')\n","\n","validation.reset_index(inplace=True)\n","validation.drop(['index'], inplace=True, axis=1)\n","validation.to_csv('/content/validation_dataset_for_segmentation_model.csv')\n","\n","print('Train\\n====================================')\n","print('shape: ', train.shape)\n","display(train.head())\n","\n","print('Validation\\n====================================')\n","print('shape: ', validation.shape)\n","display(validation.head())"],"metadata":{"id":"ErAZU7bIJjGU","colab":{"base_uri":"https://localhost:8080/","height":677},"executionInfo":{"status":"ok","timestamp":1698853571339,"user_tz":-330,"elapsed":125,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"3a59f29c-74d6-41cb-9d6a-810e9a4d4088"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train\n","====================================\n","shape:  (2302, 7)\n"]},{"output_type":"display_data","data":{"text/plain":["                                              img1  \\\n","0     gallery/ambitious-tough-teal-from-asgard.jpg   \n","1          gallery/busy-aspiring-oryx-of-peace.jpg   \n","2        gallery/fine-shrewd-oarfish-of-genius.jpg   \n","3  gallery/dainty-stork-of-hypothetical-health.jpg   \n","4      gallery/bold-nickel-gecko-of-reputation.jpg   \n","\n","                                                img2  label  img1_masked  \\\n","0            gallery/busy-aspiring-oryx-of-peace.jpg      1            1   \n","1  gallery/vigorous-upbeat-barracuda-of-strength.jpg      1            1   \n","2   gallery/alluring-quiet-peacock-of-renovation.jpg      1            1   \n","3      gallery/kind-opalescent-lion-of-adventure.jpg      1            1   \n","4  gallery/colorful-mutant-boobook-of-popularity.jpg      1            1   \n","\n","   img2_masked img1_category img2_category  \n","0            1         shoes         shoes  \n","1            1         shoes         shoes  \n","2            1         shoes         shoes  \n","3            1         shoes         shoes  \n","4            1    basketball    basketball  "],"text/html":["\n","  <div id=\"df-d689035d-b3e5-497e-9c1c-fb1e58acf320\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>img1</th>\n","      <th>img2</th>\n","      <th>label</th>\n","      <th>img1_masked</th>\n","      <th>img2_masked</th>\n","      <th>img1_category</th>\n","      <th>img2_category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>gallery/ambitious-tough-teal-from-asgard.jpg</td>\n","      <td>gallery/busy-aspiring-oryx-of-peace.jpg</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>shoes</td>\n","      <td>shoes</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>gallery/busy-aspiring-oryx-of-peace.jpg</td>\n","      <td>gallery/vigorous-upbeat-barracuda-of-strength.jpg</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>shoes</td>\n","      <td>shoes</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>gallery/fine-shrewd-oarfish-of-genius.jpg</td>\n","      <td>gallery/alluring-quiet-peacock-of-renovation.jpg</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>shoes</td>\n","      <td>shoes</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>gallery/dainty-stork-of-hypothetical-health.jpg</td>\n","      <td>gallery/kind-opalescent-lion-of-adventure.jpg</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>shoes</td>\n","      <td>shoes</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>gallery/bold-nickel-gecko-of-reputation.jpg</td>\n","      <td>gallery/colorful-mutant-boobook-of-popularity.jpg</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>basketball</td>\n","      <td>basketball</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d689035d-b3e5-497e-9c1c-fb1e58acf320')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-d689035d-b3e5-497e-9c1c-fb1e58acf320 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-d689035d-b3e5-497e-9c1c-fb1e58acf320');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-8f00cf9a-de6f-4f5c-b481-fd19f714b26e\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8f00cf9a-de6f-4f5c-b481-fd19f714b26e')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-8f00cf9a-de6f-4f5c-b481-fd19f714b26e button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Validation\n","====================================\n","shape:  (569, 7)\n"]},{"output_type":"display_data","data":{"text/plain":["                                               img1  \\\n","0    gallery/abiding-debonair-viper-of-downpour.jpg   \n","1             gallery/first-clay-mule-of-aurora.jpg   \n","2     gallery/burrowing-carmine-shrew-of-growth.jpg   \n","3    gallery/fearless-cherry-sawfly-of-sunshine.jpg   \n","4  gallery/perky-mutant-dachshund-of-witchcraft.jpg   \n","\n","                                                img2  label  img1_masked  \\\n","0            gallery/modest-proud-ant-of-triumph.jpg      1            1   \n","1      gallery/judicious-evasive-deer-of-purring.jpg      1            0   \n","2         gallery/tacky-berserk-earwig-of-luxury.jpg      1            1   \n","3     gallery/benign-ludicrous-aardwark-of-karma.jpg      1            1   \n","4  gallery/vegan-sparkling-dragonfly-of-performan...      1            0   \n","\n","   img2_masked img1_category img2_category  \n","0            1         shoes         shoes  \n","1            1         shoes         shoes  \n","2            1         shoes         shoes  \n","3            1    sunglasses    sunglasses  \n","4            1         shoes         shoes  "],"text/html":["\n","  <div id=\"df-c70bea88-b28a-4417-a0e5-7d2e72f951b2\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>img1</th>\n","      <th>img2</th>\n","      <th>label</th>\n","      <th>img1_masked</th>\n","      <th>img2_masked</th>\n","      <th>img1_category</th>\n","      <th>img2_category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>gallery/abiding-debonair-viper-of-downpour.jpg</td>\n","      <td>gallery/modest-proud-ant-of-triumph.jpg</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>shoes</td>\n","      <td>shoes</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>gallery/first-clay-mule-of-aurora.jpg</td>\n","      <td>gallery/judicious-evasive-deer-of-purring.jpg</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>shoes</td>\n","      <td>shoes</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>gallery/burrowing-carmine-shrew-of-growth.jpg</td>\n","      <td>gallery/tacky-berserk-earwig-of-luxury.jpg</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>shoes</td>\n","      <td>shoes</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>gallery/fearless-cherry-sawfly-of-sunshine.jpg</td>\n","      <td>gallery/benign-ludicrous-aardwark-of-karma.jpg</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>sunglasses</td>\n","      <td>sunglasses</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>gallery/perky-mutant-dachshund-of-witchcraft.jpg</td>\n","      <td>gallery/vegan-sparkling-dragonfly-of-performan...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>shoes</td>\n","      <td>shoes</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c70bea88-b28a-4417-a0e5-7d2e72f951b2')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-c70bea88-b28a-4417-a0e5-7d2e72f951b2 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-c70bea88-b28a-4417-a0e5-7d2e72f951b2');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-31c36d18-8733-4e49-88ec-52f961a0d933\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-31c36d18-8733-4e49-88ec-52f961a0d933')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-31c36d18-8733-4e49-88ec-52f961a0d933 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{}}]},{"cell_type":"code","source":["!cp '/content/train_dataset_for_segmentation_model.csv' '/content/gdrive/MyDrive/'\n","!cp '/content/validation_dataset_for_segmentation_model.csv' '/content/gdrive/MyDrive/'"],"metadata":{"id":"KKiO5dQ4iVpp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def enhance_class_name(class_names: List[str]) -> List[str]:\n","    return [\n","        f\"all {class_name}s\"\n","        for class_name\n","        in class_names\n","    ]\n","\n","def segment(sam_predictor: SamPredictor, image: np.ndarray, xyxy: np.ndarray) -> np.ndarray:\n","    sam_predictor.set_image(image)\n","    result_masks = []\n","    for box in xyxy:\n","        masks, scores, logits = sam_predictor.predict(\n","            box=box,\n","            multimask_output=True\n","        )\n","        index = np.argmax(scores)\n","        result_masks.append(masks[index])\n","    return np.array(result_masks)\n","\n","def detect_masks(img_path, category):\n","    if category is not None:\n","      CLASSES = [category]\n","    else:\n","      CLASSES = ['products']\n","    BOX_TRESHOLD = 0.4\n","    TEXT_TRESHOLD = 0.25\n","    image = cv2.imread(img_path)\n","    detections = grounding_dino_model.predict_with_classes(\n","        image=image,\n","        classes=enhance_class_name(class_names=CLASSES),\n","        box_threshold=BOX_TRESHOLD,\n","        text_threshold=TEXT_TRESHOLD\n","    )\n","    detections = detections[detections.class_id != None]\n","    detections.mask = segment(\n","        sam_predictor=sam_predictor,\n","        image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB),\n","        xyxy=detections.xyxy\n","    )\n","    return detections.mask, detections.xyxy\n","\n","def get_mask_and_inverse_mask(img_path, category):\n","  masks, bboxes = detect_masks(img_path, category)\n","  image = read_image(img_path)\n","\n","  if len(masks) >= 1:\n","    masks_np_arr = {}\n","\n","    for i in range(len(masks)):\n","      mask = masks[i:i+1, :]\n","      mask = mask.astype(int)\n","      mask = np.repeat(mask, 3, axis = 0)\n","      # mask = np.where(mask==0, 0, 1)\n","      # mask = mask.astype('uint8')\n","      masks_np_arr[i] = mask\n","\n","    final_mask = None\n","    if len(masks_np_arr) > 1:\n","      for key, mask in masks_np_arr.items():\n","        if final_mask is None:\n","          final_mask = mask\n","        else:\n","          final_mask = np.add(final_mask, mask)\n","      final_mask = np.where(final_mask > 0, 1, 0)\n","      final_inverse_mask = np.where(final_mask==0, 1, 0)\n","      # final_mask = final_mask.astype('uint8')\n","      # final_inverse_mask = final_inverse_mask.astype('uint8')\n","    else:\n","      final_mask = masks_np_arr[0]\n","      final_inverse_mask = np.where(final_mask==0, 1, 0)\n","      # final_inverse_mask = final_inverse_mask.astype('uint8')\n","\n","    image_ = image.numpy()\n","\n","    # final_mask = final_mask.astype('int64')\n","    final_mask = np.where(final_mask == 0, -1, 1)\n","    masked = np.multiply(image_, final_mask)\n","    masked = np.where(masked < 0, 240, masked)\n","    masked = masked.astype('uint8')\n","\n","    # final_inverse_mask = final_inverse_mask.astype('int64')\n","    final_inverse_mask = np.where(final_inverse_mask == 0, -1, 1)\n","    inverse_masked = np.multiply(image_, final_inverse_mask)\n","    inverse_masked = np.where(inverse_masked < 0, 240, inverse_masked)\n","    inverse_masked = inverse_masked.astype('uint8')\n","\n","    masked = torch.tensor(masked)\n","    inverse_masked = torch.tensor(inverse_masked)\n","\n","    # calculate bounding box parameters for cropping\n","    x = []\n","    y = []\n","    for bbox in bboxes:\n","      x.append(bbox[0])\n","      x.append(bbox[2])\n","      y.append(bbox[1])\n","      y.append(bbox[3])\n","    min_x = math.floor(min(x))\n","    min_y = math.floor(min(y))\n","    max_x = math.ceil(max(x))\n","    max_y = math.ceil(max(y))\n","\n","    return image, masked, inverse_masked, (min_x, min_y, max_x - min_x, max_y - min_y)\n","  else:\n","    return image, None, None, None"],"metadata":{"id":"iKfHx6dxdk4n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_img_color_convert(img_path):\n","  \"\"\"\n","  This function returns an image in the format RGB when the path\n","  to the image is given\n","\n","  Parameters\n","  ----------\n","  img_path: string\n","      The path to the image\n","\n","  Returns\n","  -------\n","  RGB image of the image in the specified path\n","  \"\"\"\n","  return cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n","\n","def get_img(img_path):\n","  \"\"\"\n","  This function returns an image in the format RGB when the path\n","  to the image is given\n","\n","  Parameters\n","  ----------\n","  img_path: string\n","      The path to the image\n","\n","  Returns\n","  -------\n","  RGB image of the image in the specified path\n","  \"\"\"\n","  return cv2.imread(img_path)\n","\n","def draw_bounding_box_and_save(img_path, x, y, w, h):\n","  \"\"\"\n","  This function draw a bounding box specified by x, y, w, h parameters\n","  and save the image in the '/content/temp-imgs/' folder\n","\n","  Parameters\n","  ----------\n","  img_path: string\n","      The path to original image which the bounding box should be drawn\n","  x: integer\n","      x-coordinate of top-left corner of bounding box\n","  y: integer\n","      y-coordinate of top-left corner of bounding box\n","  w: integer\n","      width of bounding box in pixels\n","  h: integer\n","      height of bounding box in pixels\n","  \"\"\"\n","  image = get_img(img_path)\n","  image = image[y:y+h, x:x+w]\n","  # cv2.rectangle(image, (x, y), (x + w, y + h), (0,255,255), 4)\n","\n","  Path(\"/content/temp-imgs\").mkdir(parents=True, exist_ok=True)\n","\n","  filepath = '/content/temp-imgs/' + img_path.split('/')[-1]\n","  cv2.imwrite(filepath, image)\n","  return filepath"],"metadata":{"id":"7hFvSIvhBvEb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_transform = T.Compose([T.ToPILImage(),\n","                           T.Resize(size=(224, 224),\n","                                    interpolation=T.InterpolationMode.BICUBIC,\n","                                    antialias=True),\n","                           T.ToTensor(),\n","                           T.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n","                                       std=(0.26862954, 0.26130258, 0.27577711))])"],"metadata":{"id":"9hpTO9tLCCrM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TrainValidationDataset(Dataset):\n","    def __init__(self, annotations_file, img_dir, indices, save_folder,\n","                 transform=None,\n","                 crop_transform=None,\n","                 target_transform=None,\n","                 crop_to_bbox=False,\n","                 crop_to_bbox_detections=False,\n","                 mode='save'):\n","        self.img_labels = pd.read_csv(annotations_file)\n","        self.img_dir = img_dir\n","        self.indices = indices\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        self.crop_to_bbox = crop_to_bbox\n","        self.crop_to_bbox_detections = crop_to_bbox_detections\n","        self.crop_transform = crop_transform\n","        self.mode = mode\n","        self.save_folder = save_folder\n","\n","        folder_name1 = f'train/gallery'\n","        if not os.path.exists('/content/' + folder_name1):\n","          os.makedirs('/content/' + folder_name1)\n","\n","        folder_name2 = f'validate/gallery'\n","        if not os.path.exists('/content/' + folder_name2):\n","          os.makedirs('/content/' + folder_name2)\n","\n","    def __len__(self):\n","        return len(self.img_labels)\n","        # return 2\n","\n","    def __getitem__(self, idx):\n","        img1_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, self.indices[0]])\n","        img2_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, self.indices[1]])\n","\n","        if self.mode == 'save':\n","          label = self.img_labels.iloc[idx, self.indices[2]]\n","          img1_mask = self.img_labels.iloc[idx, self.indices[3]]\n","          img2_mask = self.img_labels.iloc[idx, self.indices[4]]\n","          img1_cat = self.img_labels.iloc[idx, self.indices[5]]\n","          img2_cat = self.img_labels.iloc[idx, self.indices[6]]\n","\n","          if pd.isna(img1_cat):\n","            img1_cat = None\n","          if pd.isna(img2_cat):\n","            img2_cat = None\n","\n","          masked1_file = Path(f'/content/{self.save_folder}/'+ self.img_labels.iloc[idx, self.indices[0]] + '.pt')\n","          masked2_file = Path(f'/content/{self.save_folder}/'+ self.img_labels.iloc[idx, self.indices[1]] + '.pt')\n","\n","          if masked1_file.exists() and masked2_file.exists():\n","            return 0\n","          else:\n","            if not masked1_file.exists():\n","              image1, masked1, inverse_masked1, bbox1 = get_mask_and_inverse_mask('/content/' + img1_path, img1_cat)\n","            if not masked2_file.exists():\n","              image2, masked2, inverse_masked2, bbox2 = get_mask_and_inverse_mask('/content/' + img2_path, img2_cat)\n","\n","            if not masked1_file.exists() and img1_mask == 0:\n","              masked1 = inverse_masked1\n","\n","            if not masked2_file.exists() and img2_mask == 0:\n","              masked2 = inverse_masked2\n","\n","            if self.transform and not self.crop_to_bbox_detections:\n","                image1 = self.transform(image1)\n","                image2 = self.transform(image2)\n","                if not masked1_file.exists() and masked1 is not None:\n","                  masked1 = self.transform(masked1)\n","                if not masked2_file.exists() and masked2 is not None:\n","                  masked2 = self.transform(masked2)\n","            elif self.crop_transform and self.crop_to_bbox_detections:\n","                if not masked1_file.exists() and masked1 is not None and bbox1 is not None:\n","                  x = bbox1[0]\n","                  y = bbox1[1]\n","                  w = bbox1[2]\n","                  h = bbox1[3]\n","                  image1 = self.crop_transform(image1, x, y, w, h)\n","                  masked1 = self.crop_transform(masked1, x, y, w, h)\n","                elif not masked1_file.exists():\n","                  if self.transform:\n","                    image1 = self.transform(image1)\n","\n","                if not masked2_file.exists() and masked2 is not None and bbox2 is not None:\n","                  x = bbox2[0]\n","                  y = bbox2[1]\n","                  w = bbox2[2]\n","                  h = bbox2[3]\n","                  image2 = self.crop_transform(image2, x, y, w, h)\n","                  masked2 = self.crop_transform(masked2, x, y, w, h)\n","                elif not masked2_file.exists():\n","                  if self.transform:\n","                    image2 = self.transform(image2)\n","\n","            if self.target_transform:\n","                label = self.target_transform(label)\n","\n","            if not masked1_file.exists() and masked1 is None:\n","              masked1 = image1\n","\n","            if not masked2_file.exists() and masked2 is None:\n","              masked2 = image2\n","\n","            num_saved = 0\n","            masked1_file = Path(f'/content/{self.save_folder}/'+ self.img_labels.iloc[idx, self.indices[0]] + '.pt')\n","            masked2_file = Path(f'/content/{self.save_folder}/'+ self.img_labels.iloc[idx, self.indices[1]] + '.pt')\n","            if not masked1_file.exists():\n","              num_saved += 1\n","              torch.save(masked1, f'/content/{self.save_folder}/'+ self.img_labels.iloc[idx, self.indices[0]] + '.pt')\n","            if not masked2_file.exists():\n","              num_saved += 1\n","              torch.save(masked2, f'/content/{self.save_folder}/'+ self.img_labels.iloc[idx, self.indices[1]] + '.pt')\n","\n","            return num_saved\n","        else:\n","          masked1 = torch.load(img1_path + '.pt')\n","          masked2 = torch.load(img2_path + '.pt')\n","          label = self.img_labels.iloc[idx, self.indices[2]]\n","\n","          return masked1, masked2, label\n","\n","def image_transformation(image):\n","    images = img_transform(image)\n","    #images = img_transform(F.to_pil_image(image))\n","    return images\n","\n","def image_transform_with_bbox_crop(image, x, y, w, h):\n","    images = F.crop(image, y, x, h, w)\n","    images = img_transform(images)\n","    return images"],"metadata":{"id":"LHPCXuijdm-B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!rm -rf '/content/train'\n","!rm -rf '/content/validate'"],"metadata":{"id":"kWawqireUuFd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = TrainValidationDataset(\"/content/train_dataset_for_segmentation_model.csv\", \"\", [1, 2, 3, 4, 5, 6, 7], 'train', transform=image_transformation, crop_transform=image_transform_with_bbox_crop, crop_to_bbox_detections=True)\n","validation_dataset = TrainValidationDataset(\"/content/validation_dataset_for_segmentation_model.csv\", \"\", [1, 2, 3, 4, 5, 6, 7], 'validate', transform=image_transformation, crop_transform=image_transform_with_bbox_crop, crop_to_bbox_detections=True)\n","train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","validation_dataloader = DataLoader(validation_dataset, batch_size=32, shuffle=True)\n","\n","count = 0\n","for i, num in enumerate(tqdm(train_dataloader)):\n","  count += np.sum(num.numpy())\n","print('number of unique saved images: ', count)\n","\n","count = 0\n","for i, num in enumerate(tqdm(validation_dataloader)):\n","  count += np.sum(num.numpy())\n","print('number of unique saved images: ', count)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"snvEKnya71VE","executionInfo":{"status":"ok","timestamp":1698854833578,"user_tz":-330,"elapsed":1259925,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"13566a84-3a99-45bd-9db4-db538e3e81fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 72/72 [14:15<00:00, 11.89s/it]\n"]},{"output_type":"stream","name":"stdout","text":["number of unique saved images:  994\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 18/18 [06:45<00:00, 22.53s/it]"]},{"output_type":"stream","name":"stdout","text":["number of unique saved images:  477\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["%%capture\n","!zip -r '/content/train_segment.zip' '/content/train'"],"metadata":{"id":"wpSnmXUHebJS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","!zip -r '/content/validate_segment.zip' '/content/validate'"],"metadata":{"id":"Qrrj0mYqeoJC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp '/content/train_segment.zip' '/content/gdrive/MyDrive/train_segment.zip'\n","!cp '/content/validate_segment.zip' '/content/gdrive/MyDrive/validate_segment.zip'"],"metadata":{"id":"tC6BAWCie4fO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","!unzip '/content/gdrive/MyDrive/train_segment.zip' -d '/'"],"metadata":{"id":"akzEJmYRgtmE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","!unzip '/content/gdrive/MyDrive/validate_segment.zip' -d '/'"],"metadata":{"id":"8jCtw4g2gfiW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp '/content/gdrive/MyDrive/train_dataset_for_segmentation_model.csv' '/content/'\n","!cp '/content/gdrive/MyDrive/validation_dataset_for_segmentation_model.csv' '/content/'"],"metadata":{"id":"TxO5itibiio7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["del grounding_dino_model\n","del sam_predictor\n","del sam\n","\n","torch.cuda.empty_cache()"],"metadata":{"id":"_a_LHYoY-pmn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w1vQ3fKR_j3v","executionInfo":{"status":"ok","timestamp":1698855780104,"user_tz":-330,"elapsed":22,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"823d4c86-2af0-4139-abcf-00d0fe87e7e8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Nov  1 16:22:59 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P0    23W / 300W |      0MiB / 16384MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["class EarlyStopping:\n","    def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n","        self.patience = patience\n","        self.min_delta = min_delta\n","        self.restore_best_weights = restore_best_weights\n","        self.best_model = None\n","        self.best_loss = None\n","        self.counter = 0\n","        self.status = \"\"\n","\n","    def __call__(self, model, val_loss):\n","        if self.best_loss is None:\n","            self.best_loss = val_loss\n","            self.best_model = copy.deepcopy(model.state_dict())\n","        elif self.best_loss - val_loss >= self.min_delta:\n","            self.best_model = copy.deepcopy(model.state_dict())\n","            self.best_loss = val_loss\n","            self.counter = 0\n","            self.status = f\"Improvement found, counter reset to {self.counter}\"\n","        else:\n","            self.counter += 1\n","            self.status = f\"No improvement in the last {self.counter} epochs\"\n","            if self.counter >= self.patience:\n","                self.status = f\"Early stopping triggered after {self.counter} epochs.\"\n","                if self.restore_best_weights:\n","                    model.load_state_dict(self.best_model)\n","                return True\n","        return False\n","\n","    def save_the_best_model(self, path):\n","      torch.save(self.best_model, path)\n","      return self.best_loss"],"metadata":{"id":"UnCP4MQkeDlm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vit_model = open_clip.create_model_and_transforms('ViT-H-14', None)[0].visual\n","vit_model.load_state_dict(torch.load('/content/pretrained/model3.pt'))\n","vit_model"],"metadata":{"id":"y93Az-fdZ7_Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698851589415,"user_tz":-330,"elapsed":20421,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"dc562ed5-a8bf-4f98-d6ad-3534857dbc40"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["VisionTransformer(\n","  (conv1): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14), bias=False)\n","  (patch_dropout): Identity()\n","  (ln_pre): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","  (transformer): Transformer(\n","    (resblocks): ModuleList(\n","      (0-31): 32 x ResidualAttentionBlock(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","        )\n","        (ls_1): Identity()\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n","          (gelu): GELU(approximate='none')\n","          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n","        )\n","        (ls_2): Identity()\n","      )\n","    )\n","  )\n","  (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",")"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","source":["for name, para in vit_model.named_parameters():\n","  print(\"-\"*20)\n","  print(f\"name: {name}\")"],"metadata":{"id":"DJ9esnRXbsf_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698851589415,"user_tz":-330,"elapsed":32,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"8b23e8e9-8c2c-4166-abf6-090b92203077"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------\n","name: class_embedding\n","--------------------\n","name: positional_embedding\n","--------------------\n","name: proj\n","--------------------\n","name: conv1.weight\n","--------------------\n","name: ln_pre.weight\n","--------------------\n","name: ln_pre.bias\n","--------------------\n","name: transformer.resblocks.0.ln_1.weight\n","--------------------\n","name: transformer.resblocks.0.ln_1.bias\n","--------------------\n","name: transformer.resblocks.0.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.0.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.0.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.0.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.0.ln_2.weight\n","--------------------\n","name: transformer.resblocks.0.ln_2.bias\n","--------------------\n","name: transformer.resblocks.0.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.0.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.0.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.0.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.1.ln_1.weight\n","--------------------\n","name: transformer.resblocks.1.ln_1.bias\n","--------------------\n","name: transformer.resblocks.1.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.1.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.1.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.1.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.1.ln_2.weight\n","--------------------\n","name: transformer.resblocks.1.ln_2.bias\n","--------------------\n","name: transformer.resblocks.1.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.1.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.1.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.1.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.2.ln_1.weight\n","--------------------\n","name: transformer.resblocks.2.ln_1.bias\n","--------------------\n","name: transformer.resblocks.2.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.2.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.2.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.2.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.2.ln_2.weight\n","--------------------\n","name: transformer.resblocks.2.ln_2.bias\n","--------------------\n","name: transformer.resblocks.2.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.2.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.2.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.2.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.3.ln_1.weight\n","--------------------\n","name: transformer.resblocks.3.ln_1.bias\n","--------------------\n","name: transformer.resblocks.3.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.3.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.3.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.3.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.3.ln_2.weight\n","--------------------\n","name: transformer.resblocks.3.ln_2.bias\n","--------------------\n","name: transformer.resblocks.3.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.3.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.3.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.3.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.4.ln_1.weight\n","--------------------\n","name: transformer.resblocks.4.ln_1.bias\n","--------------------\n","name: transformer.resblocks.4.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.4.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.4.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.4.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.4.ln_2.weight\n","--------------------\n","name: transformer.resblocks.4.ln_2.bias\n","--------------------\n","name: transformer.resblocks.4.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.4.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.4.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.4.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.5.ln_1.weight\n","--------------------\n","name: transformer.resblocks.5.ln_1.bias\n","--------------------\n","name: transformer.resblocks.5.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.5.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.5.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.5.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.5.ln_2.weight\n","--------------------\n","name: transformer.resblocks.5.ln_2.bias\n","--------------------\n","name: transformer.resblocks.5.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.5.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.5.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.5.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.6.ln_1.weight\n","--------------------\n","name: transformer.resblocks.6.ln_1.bias\n","--------------------\n","name: transformer.resblocks.6.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.6.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.6.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.6.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.6.ln_2.weight\n","--------------------\n","name: transformer.resblocks.6.ln_2.bias\n","--------------------\n","name: transformer.resblocks.6.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.6.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.6.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.6.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.7.ln_1.weight\n","--------------------\n","name: transformer.resblocks.7.ln_1.bias\n","--------------------\n","name: transformer.resblocks.7.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.7.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.7.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.7.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.7.ln_2.weight\n","--------------------\n","name: transformer.resblocks.7.ln_2.bias\n","--------------------\n","name: transformer.resblocks.7.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.7.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.7.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.7.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.8.ln_1.weight\n","--------------------\n","name: transformer.resblocks.8.ln_1.bias\n","--------------------\n","name: transformer.resblocks.8.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.8.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.8.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.8.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.8.ln_2.weight\n","--------------------\n","name: transformer.resblocks.8.ln_2.bias\n","--------------------\n","name: transformer.resblocks.8.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.8.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.8.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.8.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.9.ln_1.weight\n","--------------------\n","name: transformer.resblocks.9.ln_1.bias\n","--------------------\n","name: transformer.resblocks.9.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.9.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.9.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.9.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.9.ln_2.weight\n","--------------------\n","name: transformer.resblocks.9.ln_2.bias\n","--------------------\n","name: transformer.resblocks.9.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.9.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.9.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.9.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.10.ln_1.weight\n","--------------------\n","name: transformer.resblocks.10.ln_1.bias\n","--------------------\n","name: transformer.resblocks.10.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.10.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.10.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.10.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.10.ln_2.weight\n","--------------------\n","name: transformer.resblocks.10.ln_2.bias\n","--------------------\n","name: transformer.resblocks.10.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.10.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.10.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.10.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.11.ln_1.weight\n","--------------------\n","name: transformer.resblocks.11.ln_1.bias\n","--------------------\n","name: transformer.resblocks.11.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.11.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.11.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.11.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.11.ln_2.weight\n","--------------------\n","name: transformer.resblocks.11.ln_2.bias\n","--------------------\n","name: transformer.resblocks.11.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.11.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.11.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.11.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.12.ln_1.weight\n","--------------------\n","name: transformer.resblocks.12.ln_1.bias\n","--------------------\n","name: transformer.resblocks.12.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.12.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.12.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.12.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.12.ln_2.weight\n","--------------------\n","name: transformer.resblocks.12.ln_2.bias\n","--------------------\n","name: transformer.resblocks.12.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.12.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.12.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.12.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.13.ln_1.weight\n","--------------------\n","name: transformer.resblocks.13.ln_1.bias\n","--------------------\n","name: transformer.resblocks.13.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.13.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.13.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.13.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.13.ln_2.weight\n","--------------------\n","name: transformer.resblocks.13.ln_2.bias\n","--------------------\n","name: transformer.resblocks.13.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.13.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.13.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.13.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.14.ln_1.weight\n","--------------------\n","name: transformer.resblocks.14.ln_1.bias\n","--------------------\n","name: transformer.resblocks.14.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.14.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.14.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.14.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.14.ln_2.weight\n","--------------------\n","name: transformer.resblocks.14.ln_2.bias\n","--------------------\n","name: transformer.resblocks.14.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.14.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.14.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.14.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.15.ln_1.weight\n","--------------------\n","name: transformer.resblocks.15.ln_1.bias\n","--------------------\n","name: transformer.resblocks.15.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.15.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.15.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.15.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.15.ln_2.weight\n","--------------------\n","name: transformer.resblocks.15.ln_2.bias\n","--------------------\n","name: transformer.resblocks.15.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.15.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.15.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.15.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.16.ln_1.weight\n","--------------------\n","name: transformer.resblocks.16.ln_1.bias\n","--------------------\n","name: transformer.resblocks.16.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.16.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.16.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.16.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.16.ln_2.weight\n","--------------------\n","name: transformer.resblocks.16.ln_2.bias\n","--------------------\n","name: transformer.resblocks.16.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.16.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.16.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.16.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.17.ln_1.weight\n","--------------------\n","name: transformer.resblocks.17.ln_1.bias\n","--------------------\n","name: transformer.resblocks.17.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.17.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.17.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.17.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.17.ln_2.weight\n","--------------------\n","name: transformer.resblocks.17.ln_2.bias\n","--------------------\n","name: transformer.resblocks.17.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.17.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.17.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.17.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.18.ln_1.weight\n","--------------------\n","name: transformer.resblocks.18.ln_1.bias\n","--------------------\n","name: transformer.resblocks.18.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.18.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.18.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.18.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.18.ln_2.weight\n","--------------------\n","name: transformer.resblocks.18.ln_2.bias\n","--------------------\n","name: transformer.resblocks.18.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.18.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.18.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.18.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.19.ln_1.weight\n","--------------------\n","name: transformer.resblocks.19.ln_1.bias\n","--------------------\n","name: transformer.resblocks.19.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.19.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.19.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.19.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.19.ln_2.weight\n","--------------------\n","name: transformer.resblocks.19.ln_2.bias\n","--------------------\n","name: transformer.resblocks.19.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.19.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.19.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.19.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.20.ln_1.weight\n","--------------------\n","name: transformer.resblocks.20.ln_1.bias\n","--------------------\n","name: transformer.resblocks.20.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.20.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.20.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.20.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.20.ln_2.weight\n","--------------------\n","name: transformer.resblocks.20.ln_2.bias\n","--------------------\n","name: transformer.resblocks.20.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.20.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.20.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.20.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.21.ln_1.weight\n","--------------------\n","name: transformer.resblocks.21.ln_1.bias\n","--------------------\n","name: transformer.resblocks.21.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.21.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.21.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.21.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.21.ln_2.weight\n","--------------------\n","name: transformer.resblocks.21.ln_2.bias\n","--------------------\n","name: transformer.resblocks.21.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.21.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.21.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.21.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.22.ln_1.weight\n","--------------------\n","name: transformer.resblocks.22.ln_1.bias\n","--------------------\n","name: transformer.resblocks.22.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.22.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.22.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.22.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.22.ln_2.weight\n","--------------------\n","name: transformer.resblocks.22.ln_2.bias\n","--------------------\n","name: transformer.resblocks.22.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.22.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.22.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.22.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.23.ln_1.weight\n","--------------------\n","name: transformer.resblocks.23.ln_1.bias\n","--------------------\n","name: transformer.resblocks.23.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.23.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.23.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.23.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.23.ln_2.weight\n","--------------------\n","name: transformer.resblocks.23.ln_2.bias\n","--------------------\n","name: transformer.resblocks.23.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.23.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.23.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.23.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.24.ln_1.weight\n","--------------------\n","name: transformer.resblocks.24.ln_1.bias\n","--------------------\n","name: transformer.resblocks.24.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.24.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.24.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.24.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.24.ln_2.weight\n","--------------------\n","name: transformer.resblocks.24.ln_2.bias\n","--------------------\n","name: transformer.resblocks.24.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.24.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.24.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.24.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.25.ln_1.weight\n","--------------------\n","name: transformer.resblocks.25.ln_1.bias\n","--------------------\n","name: transformer.resblocks.25.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.25.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.25.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.25.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.25.ln_2.weight\n","--------------------\n","name: transformer.resblocks.25.ln_2.bias\n","--------------------\n","name: transformer.resblocks.25.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.25.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.25.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.25.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.26.ln_1.weight\n","--------------------\n","name: transformer.resblocks.26.ln_1.bias\n","--------------------\n","name: transformer.resblocks.26.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.26.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.26.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.26.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.26.ln_2.weight\n","--------------------\n","name: transformer.resblocks.26.ln_2.bias\n","--------------------\n","name: transformer.resblocks.26.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.26.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.26.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.26.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.27.ln_1.weight\n","--------------------\n","name: transformer.resblocks.27.ln_1.bias\n","--------------------\n","name: transformer.resblocks.27.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.27.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.27.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.27.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.27.ln_2.weight\n","--------------------\n","name: transformer.resblocks.27.ln_2.bias\n","--------------------\n","name: transformer.resblocks.27.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.27.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.27.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.27.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.28.ln_1.weight\n","--------------------\n","name: transformer.resblocks.28.ln_1.bias\n","--------------------\n","name: transformer.resblocks.28.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.28.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.28.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.28.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.28.ln_2.weight\n","--------------------\n","name: transformer.resblocks.28.ln_2.bias\n","--------------------\n","name: transformer.resblocks.28.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.28.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.28.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.28.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.29.ln_1.weight\n","--------------------\n","name: transformer.resblocks.29.ln_1.bias\n","--------------------\n","name: transformer.resblocks.29.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.29.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.29.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.29.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.29.ln_2.weight\n","--------------------\n","name: transformer.resblocks.29.ln_2.bias\n","--------------------\n","name: transformer.resblocks.29.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.29.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.29.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.29.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.30.ln_1.weight\n","--------------------\n","name: transformer.resblocks.30.ln_1.bias\n","--------------------\n","name: transformer.resblocks.30.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.30.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.30.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.30.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.30.ln_2.weight\n","--------------------\n","name: transformer.resblocks.30.ln_2.bias\n","--------------------\n","name: transformer.resblocks.30.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.30.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.30.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.30.mlp.c_proj.bias\n","--------------------\n","name: transformer.resblocks.31.ln_1.weight\n","--------------------\n","name: transformer.resblocks.31.ln_1.bias\n","--------------------\n","name: transformer.resblocks.31.attn.in_proj_weight\n","--------------------\n","name: transformer.resblocks.31.attn.in_proj_bias\n","--------------------\n","name: transformer.resblocks.31.attn.out_proj.weight\n","--------------------\n","name: transformer.resblocks.31.attn.out_proj.bias\n","--------------------\n","name: transformer.resblocks.31.ln_2.weight\n","--------------------\n","name: transformer.resblocks.31.ln_2.bias\n","--------------------\n","name: transformer.resblocks.31.mlp.c_fc.weight\n","--------------------\n","name: transformer.resblocks.31.mlp.c_fc.bias\n","--------------------\n","name: transformer.resblocks.31.mlp.c_proj.weight\n","--------------------\n","name: transformer.resblocks.31.mlp.c_proj.bias\n","--------------------\n","name: ln_post.weight\n","--------------------\n","name: ln_post.bias\n"]}]},{"cell_type":"code","source":["# del vit_model\n","torch.cuda.empty_cache()"],"metadata":{"id":"WY4Khkxjcomh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"25ZApSdLcqDV","executionInfo":{"status":"ok","timestamp":1698855782096,"user_tz":-330,"elapsed":619,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"5f03595e-6712-4360-94a5-7e07983014b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Nov  1 16:23:00 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P0    23W / 300W |      0MiB / 16384MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["# Fully connected neural network with one hidden layer\n","class SegmentationModel(nn.Module):\n","    def __init__(self):\n","        super(SegmentationModel, self).__init__()\n","\n","        vit_model = open_clip.create_model_and_transforms('ViT-H-14', None)[0].visual\n","        vit_model.load_state_dict(torch.load('/content/pretrained/model3.pt'))\n","        self.vit_model = vit_model\n","\n","    def forward(self, x1, x2):\n","      emb1 = self.vit_model(x1)\n","      emb2 = self.vit_model(x2)\n","      return emb1, emb2"],"metadata":{"id":"w1wnMlnQeiOB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def criterion(x1, x2, label, margin: float = 35.0):\n","    \"\"\"\n","    Computes Contrastive Loss\n","    \"\"\"\n","\n","    dist = torch.nn.functional.pairwise_distance(x1, x2, p=2)\n","\n","    loss = (label) * torch.pow(dist, 2) + (1 - label) * torch.pow(torch.clamp(margin - dist, min=0.0), 2)\n","    loss = torch.mean(loss)\n","\n","    return loss"],"metadata":{"id":"E9dPANvPiDz7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","# Make sure reproducability\n","np.random.seed(0)\n","torch.manual_seed(0)\n","\n","# hyperparameters\n","num_epochs = 10\n","learning_rate = 0.01\n","\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model = SegmentationModel().to(device)\n","for name, para in model.named_parameters():\n","  if 'mlp.c_proj' in name:\n","    para.requires_grad = True\n","  else:\n","    para.requires_grad = False\n","\n","# Loss and optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Early stopper initialization\n","es = EarlyStopping(patience=5)\n","\n","# LR scheduler initialization\n","scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3)\n","\n","# Initialize datasets and dataloaders\n","train_dataset = TrainValidationDataset(\"/content/train_dataset_for_segmentation_model.csv\", \"/content/train/\", [1, 2, 3, 4, 5, 6, 7], '', transform=image_transformation, crop_transform=image_transform_with_bbox_crop, crop_to_bbox_detections=True, mode='load')\n","validation_dataset = TrainValidationDataset(\"/content/validation_dataset_for_segmentation_model.csv\", \"/content/validate/\", [1, 2, 3, 4, 5, 6, 7], '', transform=image_transformation, crop_transform=image_transform_with_bbox_crop, crop_to_bbox_detections=True, mode='load')\n","train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n","validation_dataloader = DataLoader(validation_dataset, batch_size=2, shuffle=True)\n","\n","# Gather data for graphing\n","train_loss_arr = []\n","test_loss_arr = []\n","\n","# Train the model\n","n_total_steps_train = len(train_dataloader)\n","n_total_steps_validation = len(validation_dataloader)\n","# Data structure to hold the best values\n","best_model_on_loss_value = {'train_loss':1000000, 'test_loss':1000000, 'epoch':0}\n","best_model_on_accuracy_value = {'train_loss':1000000, 'test_loss':1000000, 'epoch':0}\n","for epoch in range(num_epochs):\n","    print(f'Epoch {epoch+1}')\n","    avg_train_loss = 0\n","    for i, (imgs1, imgs2, labels) in enumerate(tqdm(train_dataloader)):\n","        imgs1, imgs2, labels = imgs1.to('cuda'), imgs2.to('cuda'), labels.to('cuda')\n","\n","        # Forward pass\n","        embs1, embs2 = model(imgs1, imgs2)\n","        loss = criterion(embs1, embs2, labels)\n","\n","        # print(f'Epoch {epoch+1} Batch {i + 1}, Training Loss: {loss:.6f}')\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        avg_train_loss += loss.item()\n","    print('\\n============================================================================================')\n","    print(f'Epoch {epoch+1}, Training Loss: {(avg_train_loss / n_total_steps_train):.6f}')\n","    print('============================================================================================\\n')\n","\n","    # Validate the model\n","    # In test phase, we don't need to compute gradients (for memory efficiency)\n","    with torch.no_grad():\n","        avg_val_loss = 0\n","        n_correct = 0\n","        n_samples = 0\n","        for imgs1, imgs2, labels in tqdm(validation_dataloader):\n","            imgs1, imgs2, labels = imgs1.to('cuda'), imgs2.to('cuda'), labels.to('cuda')\n","\n","            # Forward pass\n","            embs1, embs2 = model(imgs1, imgs2)\n","            loss = criterion(embs1, embs2, labels)\n","\n","            avg_val_loss += loss\n","\n","        scheduler.step(avg_val_loss / n_total_steps_validation)\n","\n","        print('\\n============================================================================================')\n","        print(f'Epoch {epoch+1}, Validation Loss: {(avg_val_loss / n_total_steps_validation):.6f}')\n","        print('============================================================================================\\n')\n","\n","        train_loss_arr.append(avg_train_loss / n_total_steps_train)\n","        test_loss_arr.append(avg_val_loss / n_total_steps_validation)\n","\n","        if es(model, (avg_val_loss / n_total_steps_validation)):\n","            print('Early stopping criteria reached. Training stopped.')\n","            break"],"metadata":{"id":"aIhOoBBseUom","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698861471069,"user_tz":-330,"elapsed":4500698,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"eec20179-2d12-47a7-8b5e-64d88feab107"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1151/1151 [06:52<00:00,  2.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","============================================================================================\n","Epoch 1, Training Loss: 867.207379\n","============================================================================================\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 285/285 [00:38<00:00,  7.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","============================================================================================\n","Epoch 1, Validation Loss: 343.763153\n","============================================================================================\n","\n","Epoch 2\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1151/1151 [06:46<00:00,  2.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","============================================================================================\n","Epoch 2, Training Loss: 470.932440\n","============================================================================================\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 285/285 [00:38<00:00,  7.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","============================================================================================\n","Epoch 2, Validation Loss: 341.101715\n","============================================================================================\n","\n","Epoch 3\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1151/1151 [06:45<00:00,  2.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","============================================================================================\n","Epoch 3, Training Loss: 303.080586\n","============================================================================================\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 285/285 [00:38<00:00,  7.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","============================================================================================\n","Epoch 3, Validation Loss: 200.486237\n","============================================================================================\n","\n","Epoch 4\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1151/1151 [06:45<00:00,  2.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","============================================================================================\n","Epoch 4, Training Loss: 255.038769\n","============================================================================================\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 285/285 [00:38<00:00,  7.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","============================================================================================\n","Epoch 4, Validation Loss: 277.522430\n","============================================================================================\n","\n","Epoch 5\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1151/1151 [06:45<00:00,  2.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","============================================================================================\n","Epoch 5, Training Loss: 230.090111\n","============================================================================================\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 285/285 [00:38<00:00,  7.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","============================================================================================\n","Epoch 5, Validation Loss: 182.877991\n","============================================================================================\n","\n","Epoch 6\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1151/1151 [06:45<00:00,  2.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","============================================================================================\n","Epoch 6, Training Loss: 226.918054\n","============================================================================================\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 285/285 [00:38<00:00,  7.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","============================================================================================\n","Epoch 6, Validation Loss: 264.601929\n","============================================================================================\n","\n","Epoch 7\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1151/1151 [06:45<00:00,  2.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","============================================================================================\n","Epoch 7, Training Loss: 198.013016\n","============================================================================================\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 285/285 [00:38<00:00,  7.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","============================================================================================\n","Epoch 7, Validation Loss: 193.968307\n","============================================================================================\n","\n","Epoch 8\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1151/1151 [06:45<00:00,  2.84it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","============================================================================================\n","Epoch 8, Training Loss: 232.488424\n","============================================================================================\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 285/285 [00:38<00:00,  7.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","============================================================================================\n","Epoch 8, Validation Loss: 199.586929\n","============================================================================================\n","\n","Epoch 9\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1151/1151 [06:51<00:00,  2.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","============================================================================================\n","Epoch 9, Training Loss: 215.598335\n","============================================================================================\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 285/285 [00:41<00:00,  6.91it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","============================================================================================\n","Epoch 9, Validation Loss: 230.947723\n","============================================================================================\n","\n","Epoch 10\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1151/1151 [07:03<00:00,  2.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","============================================================================================\n","Epoch 10, Training Loss: 179.690862\n","============================================================================================\n","\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 285/285 [00:39<00:00,  7.25it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","============================================================================================\n","Epoch 10, Validation Loss: 189.027847\n","============================================================================================\n","\n","Early stopping criteria reached. Training stopped.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# es.save_the_best_model('/content/pretrained/model4.pt')"],"metadata":{"id":"lINu2vq6_uAZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(es.best_model, '/content/pretrained/model4.pt')"],"metadata":{"id":"BJ9abF-KDBhA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp '/content/pretrained/model4.pt' '/content/gdrive/MyDrive/visual-product-recognition-pre-trained/'"],"metadata":{"id":"N1VoSuvZF4D2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(es.best_loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rIX9iGc8DW32","executionInfo":{"status":"ok","timestamp":1698861786863,"user_tz":-330,"elapsed":558,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"b302df54-1ffb-4d1b-e1d0-8ca3f1ab15cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(182.8780, device='cuda:0')\n"]}]},{"cell_type":"code","source":["del model\n","torch.cuda.empty_cache()"],"metadata":{"id":"f-ZpWF4OEOsH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DLCTlo8NEcX5","executionInfo":{"status":"ok","timestamp":1698862066998,"user_tz":-330,"elapsed":556,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"802a11df-6529-4cfc-c21c-e171d91a23e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Nov  1 18:07:46 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   36C    P0    39W / 300W |   9076MiB / 16384MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","source":["# Test the model"],"metadata":{"id":"NB6z47UKAlF0"}},{"cell_type":"code","source":["!mkdir '/content/image_to_text_results/'\n","!cp '/content/gdrive/MyDrive/prompt_What-is-the-product-category_all_version.csv' '/content/image_to_text_results/'\n","!cp '/content/gdrive/MyDrive/prompt_What-is-the-product-category_gallery_version.csv' '/content/image_to_text_results/'\n","!cp '/content/gdrive/MyDrive/prompt_What-is-the-product-category.csv' '/content/image_to_text_results/'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zvai_31nAj5S","executionInfo":{"status":"ok","timestamp":1698863622759,"user_tz":-330,"elapsed":966,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"4296b6fa-4225-4448-f041-f05e34196b55"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘/content/image_to_text_results/’: File exists\n"]}]},{"cell_type":"code","source":["CLASSES = ['products']\n","BOX_TRESHOLD = 0.4\n","TEXT_TRESHOLD = 0.25\n","\n","def enhance_class_name(class_names: List[str]) -> List[str]:\n","    return [\n","        f\"all {class_name}s\"\n","        for class_name\n","        in class_names\n","    ]\n","\n","def segment(sam_predictor: SamPredictor, image: np.ndarray, xyxy: np.ndarray) -> np.ndarray:\n","    sam_predictor.set_image(image)\n","    result_masks = []\n","    for box in xyxy:\n","        masks, scores, logits = sam_predictor.predict(\n","            box=box,\n","            multimask_output=True\n","        )\n","        index = np.argmax(scores)\n","        result_masks.append(masks[index])\n","    return np.array(result_masks)\n","\n","def detect_masks(img_path, category):\n","    if category is not None:\n","      CLASSES = [category]\n","    else:\n","      CLASSES = ['products']\n","    image = cv2.imread(img_path)\n","    detections = grounding_dino_model.predict_with_classes(\n","        image=image,\n","        classes=enhance_class_name(class_names=CLASSES),\n","        box_threshold=BOX_TRESHOLD,\n","        text_threshold=TEXT_TRESHOLD\n","    )\n","    detections = detections[detections.class_id != None]\n","    detections.mask = segment(\n","        sam_predictor=sam_predictor,\n","        image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB),\n","        xyxy=detections.xyxy\n","    )\n","    return detections.mask, detections.xyxy\n","\n","def get_mask_and_inverse_mask(img_path, category):\n","  masks, bboxes = detect_masks(img_path, category)\n","  image = read_image(img_path)\n","\n","  if len(masks) >= 1:\n","    masks_np_arr = {}\n","\n","    for i in range(len(masks)):\n","      mask = masks[i:i+1, :]\n","      mask = mask.astype(int)\n","      mask = np.repeat(mask, 3, axis = 0)\n","      # mask = np.where(mask==0, 0, 1)\n","      # mask = mask.astype('uint8')\n","      masks_np_arr[i] = mask\n","\n","    final_mask = None\n","    if len(masks_np_arr) > 1:\n","      for key, mask in masks_np_arr.items():\n","        if final_mask is None:\n","          final_mask = mask\n","        else:\n","          final_mask = np.add(final_mask, mask)\n","      final_mask = np.where(final_mask > 0, 1, 0)\n","      final_inverse_mask = np.where(final_mask==0, 1, 0)\n","      # final_mask = final_mask.astype('uint8')\n","      # final_inverse_mask = final_inverse_mask.astype('uint8')\n","    else:\n","      final_mask = masks_np_arr[0]\n","      final_inverse_mask = np.where(final_mask==0, 1, 0)\n","      # final_inverse_mask = final_inverse_mask.astype('uint8')\n","\n","    image_ = image.numpy()\n","\n","    # final_mask = final_mask.astype('int64')\n","    final_mask = np.where(final_mask == 0, -1, 1)\n","    masked = np.multiply(image_, final_mask)\n","    masked = np.where(masked < 0, 240, masked)\n","    masked = masked.astype('uint8')\n","\n","    # final_inverse_mask = final_inverse_mask.astype('int64')\n","    final_inverse_mask = np.where(final_inverse_mask == 0, -1, 1)\n","    inverse_masked = np.multiply(image_, final_inverse_mask)\n","    inverse_masked = np.where(inverse_masked < 0, 240, inverse_masked)\n","    inverse_masked = inverse_masked.astype('uint8')\n","\n","    masked = torch.tensor(masked)\n","    inverse_masked = torch.tensor(inverse_masked)\n","\n","    # calculate bounding box parameters for cropping\n","    x = []\n","    y = []\n","    for bbox in bboxes:\n","      x.append(bbox[0])\n","      x.append(bbox[2])\n","      y.append(bbox[1])\n","      y.append(bbox[3])\n","    min_x = math.floor(min(x))\n","    min_y = math.floor(min(y))\n","    max_x = math.ceil(max(x))\n","    max_y = math.ceil(max(y))\n","\n","    return image, masked, inverse_masked, (min_x, min_y, max_x - min_x, max_y - min_y)\n","  else:\n","    return image, None, None, None"],"metadata":{"id":"cjFNl1cKApSJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_transform = T.Compose([T.ToPILImage(),\n","                           T.Resize(size=(224, 224),\n","                                    interpolation=T.InterpolationMode.BICUBIC,\n","                                    antialias=True),\n","                           T.ToTensor(),\n","                           T.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n","                                       std=(0.26862954, 0.26130258, 0.27577711))])"],"metadata":{"id":"rQkpTR-aOzQD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CustomDataset(Dataset):\n","    def __init__(self, annotations_file, category_annotations_file, img_dir, indices,\n","                 transform=None,\n","                 crop_transform=None,\n","                 target_transform=None,\n","                 crop_to_bbox=False,\n","                 crop_to_bbox_detections=False):\n","        self.img_labels = pd.read_csv(annotations_file)\n","        self.img_categories = pd.read_csv(category_annotations_file)\n","        self.img_dir = img_dir\n","        self.indices = indices\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        self.crop_to_bbox = crop_to_bbox\n","        self.crop_to_bbox_detections = crop_to_bbox_detections\n","        self.crop_transform = crop_transform\n","\n","    def __len__(self):\n","        return len(self.img_labels)\n","        # return 2\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, self.indices[0]])\n","        label = self.img_labels.iloc[idx, self.indices[1]]\n","        category = self.img_categories.loc[self.img_categories.label == 49, 'category'].values[0]\n","        if pd.isna(category):\n","          category = None\n","        if self.crop_to_bbox:\n","          x = self.img_labels.iloc[idx, self.indices[2]]\n","          y = self.img_labels.iloc[idx, self.indices[3]]\n","          w = self.img_labels.iloc[idx, self.indices[4]]\n","          h = self.img_labels.iloc[idx, self.indices[5]]\n","          cropped_img_path = draw_bounding_box_and_save('/content/' + img_path, x, y, w, h)\n","          image, masked, inverse_masked, bbox = get_mask_and_inverse_mask(cropped_img_path, category)\n","        else:\n","          image, masked, inverse_masked, bbox = get_mask_and_inverse_mask('/content/' + img_path, category)\n","        if self.transform and not self.crop_to_bbox_detections:\n","            image = self.transform(image)\n","            if masked is not None and inverse_masked is not None:\n","              masked = self.transform(masked)\n","              inverse_masked = self.transform(inverse_masked)\n","        elif self.crop_transform and self.crop_to_bbox_detections:\n","            if masked is not None and inverse_masked is not None and bbox is not None:\n","              x = bbox[0]\n","              y = bbox[1]\n","              w = bbox[2]\n","              h = bbox[3]\n","              image = self.crop_transform(image, x, y, w, h)\n","              masked = self.crop_transform(masked, x, y, w, h)\n","              inverse_masked = self.crop_transform(inverse_masked, x, y, w, h)\n","            else:\n","              if self.transform:\n","                image = self.transform(image)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        if masked is None and inverse_masked is None:\n","          masked = torch.empty(0)\n","          inverse_masked = torch.empty(0)\n","        return image, masked, inverse_masked, label, img_path\n","\n","def image_transformation(image):\n","    images = img_transform(image)\n","    #images = img_transform(F.to_pil_image(image))\n","    return images\n","\n","def image_transform_with_bbox_crop(image, x, y, w, h):\n","    images = F.crop(image, y, x, h, w)\n","    images = img_transform(images)\n","    return images"],"metadata":{"id":"2QNbrRQWAymF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_img_color_convert(img_path):\n","  \"\"\"\n","  This function returns an image in the format RGB when the path\n","  to the image is given\n","\n","  Parameters\n","  ----------\n","  img_path: string\n","      The path to the image\n","\n","  Returns\n","  -------\n","  RGB image of the image in the specified path\n","  \"\"\"\n","  return cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n","\n","def get_img(img_path):\n","  \"\"\"\n","  This function returns an image in the format RGB when the path\n","  to the image is given\n","\n","  Parameters\n","  ----------\n","  img_path: string\n","      The path to the image\n","\n","  Returns\n","  -------\n","  RGB image of the image in the specified path\n","  \"\"\"\n","  return cv2.imread(img_path)\n","\n","def draw_bounding_box_and_save(img_path, x, y, w, h):\n","  \"\"\"\n","  This function draw a bounding box specified by x, y, w, h parameters\n","  and save the image in the '/content/temp-imgs/' folder\n","\n","  Parameters\n","  ----------\n","  img_path: string\n","      The path to original image which the bounding box should be drawn\n","  x: integer\n","      x-coordinate of top-left corner of bounding box\n","  y: integer\n","      y-coordinate of top-left corner of bounding box\n","  w: integer\n","      width of bounding box in pixels\n","  h: integer\n","      height of bounding box in pixels\n","  \"\"\"\n","  image = get_img(img_path)\n","  image = image[y:y+h, x:x+w]\n","  # cv2.rectangle(image, (x, y), (x + w, y + h), (0,255,255), 4)\n","\n","  Path(\"/content/temp-imgs\").mkdir(parents=True, exist_ok=True)\n","\n","  filepath = '/content/temp-imgs/' + img_path.split('/')[-1]\n","  cv2.imwrite(filepath, image)\n","  return filepath\n","\n","def get_query_and_gallery_images_for_a_product_id(product_id):\n","  \"\"\"\n","  This function returns a list of gallery image paths and a list of query\n","  image paths correcponding to a specific product_id in testing dataset\n","\n","  Parameters\n","  ----------\n","  product_id: integer\n","\n","  Returns\n","  -------\n","  Two list of file paths of gallery and query images corresponding to product_id\n","  \"\"\"\n","  query_images_for_product_id = query.loc[query['product_id'] == product_id]\n","  query_image_list = []\n","  for index, row in query_images_for_product_id.iterrows():\n","    img_path = row['img_path']\n","    x = row['bbox_x']\n","    y = row['bbox_y']\n","    w = row['bbox_w']\n","    h = row['bbox_h']\n","    query_image_list.append(draw_bounding_box_and_save('/content/' + img_path, x, y, w, h))\n","\n","  gallery_images_for_product_id = gallery.loc[gallery['product_id'] == product_id]\n","  gallery_image_list = gallery_images_for_product_id['img_path'].tolist()\n","\n","  return query_image_list, gallery_image_list"],"metadata":{"id":"tZDciWvWUzSC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vit_model = open_clip.create_model_and_transforms('ViT-H-14', None)[0].visual\n","vit_model.load_state_dict(torch.load('/content/pretrained/model3.pt'))\n","vit_model.load_state_dict(torch.load('/content/pretrained/model4.pt'), strict=False)\n","vit_model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UNWuPnvqDzD3","executionInfo":{"status":"ok","timestamp":1698864629646,"user_tz":-330,"elapsed":31209,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"af6a64c8-cf4d-44e9-bf32-c0f405ef5413"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["VisionTransformer(\n","  (conv1): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14), bias=False)\n","  (patch_dropout): Identity()\n","  (ln_pre): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","  (transformer): Transformer(\n","    (resblocks): ModuleList(\n","      (0-31): 32 x ResidualAttentionBlock(\n","        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (attn): MultiheadAttention(\n","          (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n","        )\n","        (ls_1): Identity()\n","        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Sequential(\n","          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n","          (gelu): GELU(approximate='none')\n","          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n","        )\n","        (ls_2): Identity()\n","      )\n","    )\n","  )\n","  (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",")"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["vit_model = vit_model.to('cuda')"],"metadata":{"id":"Ud0xyPSLPjSl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gallery_dataset = CustomDataset(\"/content/original_datasets/gallery.csv\", \"/content/image_to_text_results/prompt_What-is-the-product-category_all_version.csv\", \"\", [1, 2], transform=image_transformation, crop_transform=image_transform_with_bbox_crop, crop_to_bbox_detections=True)\n","gallery_dataloader = DataLoader(gallery_dataset, batch_size=1, shuffle=False)\n","\n","df = pd.DataFrame(columns=['image_tensor_path', 'masked_image_tensor_path', 'inverse_masked_image_tensor_path', 'label', 'image_path', 'masked_similarity', 'inverse_masked_similarity'])\n","\n","folder_name = f'gallery_tensors_blip_detection_bbox_crop_trained_bt_{BOX_TRESHOLD}_tt_{TEXT_TRESHOLD}'\n","if not os.path.exists('/content/' + folder_name):\n","    os.makedirs('/content/' + folder_name)\n","\n","# graph_index = 1\n","i = 0\n","for idx, data in enumerate(tqdm(gallery_dataloader)):\n","    image, masked, inverse_masked, label, img_path = data\n","\n","    # fig = plt.figure(figsize=(8, 8))\n","\n","    # proc_img = F.to_pil_image(masked[0])\n","    # plt.subplot(2, 2, graph_index), plt.imshow(proc_img)\n","\n","    # proc_img = F.to_pil_image(inverse_masked[0])\n","    # plt.subplot(2, 2, graph_index + 1), plt.imshow(proc_img)\n","\n","    # graph_index += 2\n","\n","    image = image.to('cuda')\n","\n","    if masked.nelement() != 0 and masked.nelement() != 0:\n","      masked = masked.to('cuda')\n","      inverse_masked = inverse_masked.to('cuda')\n","\n","    with torch.no_grad():\n","        output_image = vit_model(image)\n","        embeddings_image = output_image.cpu()\n","        if masked.nelement() != 0 and masked.nelement() != 0:\n","          output_masked_image = vit_model(masked)\n","          output_inverse_masked_image = vit_model(inverse_masked)\n","          embeddings_masked_image = output_masked_image.cpu()\n","          embeddings_inverse_masked_image = output_inverse_masked_image.cpu()\n","\n","    file_path_image = f'{folder_name}/image_{idx}_{i}.pt'\n","    torch.save(embeddings_image[0], '/content/' + file_path_image)\n","    if masked.nelement() != 0 and masked.nelement() != 0:\n","      file_path_masked_image = f'{folder_name}/masked_image_{idx}_{i}.pt'\n","      file_path_inverse_masked_image = f'{folder_name}/inverse_masked_image_{idx}_{i}.pt'\n","      torch.save(embeddings_masked_image[0], '/content/' + file_path_masked_image)\n","      torch.save(embeddings_inverse_masked_image[0], '/content/' + file_path_inverse_masked_image)\n","\n","      euclidean_masked = torch.nn.functional.pairwise_distance(embeddings_image, embeddings_masked_image, p=2)\n","      euclidean_inverse_masked = torch.nn.functional.pairwise_distance(embeddings_image, embeddings_inverse_masked_image, p=2)\n","\n","      df.loc[len(df)] = [file_path_image, file_path_masked_image, file_path_inverse_masked_image, label.item(), img_path[0], euclidean_masked.item(), euclidean_inverse_masked.item()]\n","    else:\n","      df.loc[len(df)] = [file_path_image, 'None', 'None', label.item(), img_path[0], -1, -1]\n","\n","df.reset_index(inplace = True)\n","df.to_csv(f'/content/{folder_name}/gallery_tensors_blip_detection_bbox_crop_trained_bt_{BOX_TRESHOLD}_tt_{TEXT_TRESHOLD}.csv', index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e2YOChB1A-qw","executionInfo":{"status":"ok","timestamp":1698866137355,"user_tz":-330,"elapsed":1074071,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"dfdac2c4-42b7-41df-ba52-0b9a9fd338ac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1067/1067 [17:54<00:00,  1.01s/it]\n"]}]},{"cell_type":"code","source":["query_dataset = CustomDataset(\"/content/original_datasets/queries.csv\", \"/content/image_to_text_results/prompt_What-is-the-product-category_all_version.csv\", \"\", [1, 6, 2, 3, 4, 5], transform=image_transformation, crop_transform=image_transform_with_bbox_crop, crop_to_bbox_detections=True, crop_to_bbox=True)\n","query_dataloader = DataLoader(query_dataset, batch_size=1, shuffle=False)\n","\n","df = pd.DataFrame(columns=['image_tensor_path', 'masked_image_tensor_path', 'inverse_masked_image_tensor_path', 'label', 'image_path', 'masked_similarity', 'inverse_masked_similarity'])\n","\n","folder_name = f'query_tensors_blip_detection_bbox_crop_trained_bt_{BOX_TRESHOLD}_tt_{TEXT_TRESHOLD}'\n","if not os.path.exists('/content/' + folder_name):\n","    os.makedirs('/content/' + folder_name)\n","\n","# graph_index = 1\n","for idx, data in enumerate(tqdm(query_dataloader)):\n","    image, masked, inverse_masked, label, img_path = data\n","\n","    # fig = plt.figure(figsize=(8, 8))\n","\n","    # proc_img = F.to_pil_image(masked[0])\n","    # plt.subplot(2, 2, graph_index), plt.imshow(proc_img)\n","\n","    # proc_img = F.to_pil_image(inverse_masked[0])\n","    # plt.subplot(2, 2, graph_index + 1), plt.imshow(proc_img)\n","\n","    # graph_index += 2\n","\n","    image = image.to('cuda')\n","    if masked.nelement() != 0 and masked.nelement() != 0:\n","      masked = masked.to('cuda')\n","      inverse_masked = inverse_masked.to('cuda')\n","\n","    with torch.no_grad():\n","        output_image = vit_model(image)\n","        embeddings_image = output_image.cpu()\n","        if masked.nelement() != 0 and masked.nelement() != 0:\n","          output_masked_image = vit_model(masked)\n","          output_inverse_masked_image = vit_model(inverse_masked)\n","          embeddings_masked_image = output_masked_image.cpu()\n","          embeddings_inverse_masked_image = output_inverse_masked_image.cpu()\n","\n","    file_path_image = f'{folder_name}/image_{idx}_{i}.pt'\n","    torch.save(embeddings_image[0], '/content/' + file_path_image)\n","    if masked.nelement() != 0 and masked.nelement() != 0:\n","      file_path_masked_image = f'{folder_name}/masked_image_{idx}_{i}.pt'\n","      file_path_inverse_masked_image = f'{folder_name}/inverse_masked_image_{idx}_{i}.pt'\n","      torch.save(embeddings_masked_image[0], '/content/' + file_path_masked_image)\n","      torch.save(embeddings_inverse_masked_image[0], '/content/' + file_path_inverse_masked_image)\n","\n","      euclidean_masked = torch.nn.functional.pairwise_distance(embeddings_image, embeddings_masked_image, p=2)\n","      euclidean_inverse_masked = torch.nn.functional.pairwise_distance(embeddings_image, embeddings_inverse_masked_image, p=2)\n","\n","      df.loc[len(df)] = [file_path_image, file_path_masked_image, file_path_inverse_masked_image, label.item(), img_path[0], euclidean_masked.item(), euclidean_inverse_masked.item()]\n","    else:\n","      df.loc[len(df)] = [file_path_image, 'None', 'None', label.item(), img_path[0], -1, -1]\n","\n","df.reset_index(inplace = True)\n","df.to_csv(f'/content/{folder_name}/query_tensors_blip_detection_bbox_crop_trained_bt_{BOX_TRESHOLD}_tt_{TEXT_TRESHOLD}.csv', index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"brhHXEyuBCaS","executionInfo":{"status":"ok","timestamp":1698868250928,"user_tz":-330,"elapsed":1881786,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"6d1bbb87-339d-4620-ac54-db99a1534cd6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1935/1935 [31:21<00:00,  1.03it/s]\n"]}]},{"cell_type":"code","source":["%%capture\n","!zip -r '/content/gallery_tensors_blip_detection_bbox_crop_trained_bt_0.4_tt_0.25.zip' '/content/gallery_tensors_blip_detection_bbox_crop_bt_0.4_tt_0.25/'"],"metadata":{"id":"Z8o_2fvKu_Q8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","!zip -r '/content/query_tensors_blip_detection_bbox_crop_trained_bt_0.4_tt_0.25.zip' '/content/query_tensors_blip_detection_bbox_crop_bt_0.4_tt_0.25/'"],"metadata":{"id":"iT2g50nbu_Q8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!cp '/content/gallery_tensors_blip_detection_bbox_crop_trained_bt_0.4_tt_0.25.zip' '/content/gdrive/MyDrive/gallery_tensors_blip_detection_bbox_crop_trained_bt_0.4_tt_0.25.zip'\n","!cp '/content/query_tensors_blip_detection_bbox_crop_trained_bt_0.4_tt_0.25.zip' '/content/gdrive/MyDrive/query_tensors_blip_detection_bbox_crop_trained_bt_0.4_tt_0.25.zip'"],"metadata":{"id":"0AOq-Ralu_Q8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip '/content/gdrive/MyDrive/gallery_tensors_blip_detection_bbox_crop_trained_bt_0.4_tt_0.25.zip' -d '/'\n","!unzip '/content/gdrive/MyDrive/query_tensors_blip_detection_bbox_crop_trained_bt_0.4_tt_0.25.zip' -d '/'"],"metadata":{"id":"fK5qdopnB0bQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gallery_tensor_dataset_path = '/content/gallery_tensors_blip_detection_bbox_crop_trained_bt_0.4_tt_0.25/gallery_tensors_blip_detection_bbox_crop_trained_bt_0.4_tt_0.25.csv'\n","query_tensor_dataset_path = '/content/query_tensors_blip_detection_bbox_crop_trained_bt_0.4_tt_0.25/query_tensors_blip_detection_bbox_crop_trained_bt_0.4_tt_0.25.csv'"],"metadata":{"id":"v3s1J1YRB_n8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TensorDataset(Dataset):\n","    def __init__(self, annotations_file, tensor_dir, transform=None, target_transform=None):\n","        self.tensor_labels = pd.read_csv(annotations_file)\n","        self.tensor_dir = tensor_dir\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.tensor_labels)\n","\n","    def __getitem__(self, idx):\n","        image_tensor_path = os.path.join(self.tensor_dir, self.tensor_labels.iloc[idx, 1])\n","        masked_tensor_path = os.path.join(self.tensor_dir, self.tensor_labels.iloc[idx, 2])\n","        inverse_masked_tensor_path = os.path.join(self.tensor_dir, self.tensor_labels.iloc[idx, 3])\n","\n","        image = torch.load(image_tensor_path)\n","        if self.tensor_labels.iloc[idx, 2] != 'None' and self.tensor_labels.iloc[idx, 3] != 'None':\n","          masked = torch.load(masked_tensor_path)\n","          inverse_masked = torch.load(inverse_masked_tensor_path)\n","        else:\n","          masked = torch.empty(0)\n","          inverse_masked = torch.empty(0)\n","\n","        label = self.tensor_labels.iloc[idx, 4]\n","        image_path = self.tensor_labels.iloc[idx, 5]\n","\n","        euclidian_masked = self.tensor_labels.iloc[idx, 6]\n","        euclidian_inverse_masked = self.tensor_labels.iloc[idx, 7]\n","\n","        if self.transform:\n","            tensor = self.transform(tensor)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        return image, masked, inverse_masked, label, image_path, euclidian_masked, euclidian_inverse_masked"],"metadata":{"id":"KRCvJS3bCLFO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tensor_dataset = TensorDataset(gallery_tensor_dataset_path, \"/content/\")\n","tensor_dataloader = DataLoader(tensor_dataset, batch_size=1, shuffle=False)\n","\n","image_tensor_dict = {}\n","masked_tensor_dict = {}\n","label_dict = {}\n","image_path_dict = {}\n","\n","for idx, data in enumerate(tqdm(tensor_dataloader)):\n","    image, masked, inverse_masked, label, image_path, euclidian_masked, euclidian_inverse_masked = data\n","\n","    image_tensor_dict[idx] = image\n","\n","    if masked.nelement() != 0 and inverse_masked.nelement() != 0:\n","      if euclidian_masked <= euclidian_inverse_masked:\n","        masked_tensor_dict[idx] = masked\n","      else:\n","        masked_tensor_dict[idx] = inverse_masked\n","    else:\n","      masked_tensor_dict[idx] = torch.empty(0)\n","\n","    label_dict[idx] = label\n","    image_path_dict[idx] = image_path[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FQ_2hi0HCN73","executionInfo":{"status":"ok","timestamp":1698868966418,"user_tz":-330,"elapsed":2282,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"64275ec0-95e8-414b-c140-4f6009b8ef65"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1067/1067 [00:01<00:00, 575.66it/s]\n"]}]},{"cell_type":"code","source":["query_tensor_dataset = TensorDataset(query_tensor_dataset_path, \"/content/\")\n","query_tensor_dataloader = DataLoader(query_tensor_dataset, batch_size=1, shuffle=False)\n","\n","query_image_tensor_dict = {}\n","query_masked_tensor_dict = {}\n","query_label_dict = {}\n","query_image_path_dict = {}\n","\n","for idx, data in enumerate(tqdm(query_tensor_dataloader)):\n","    image, masked, inverse_masked, label, image_path, euclidian_masked, euclidian_inverse_masked = data\n","\n","    query_image_tensor_dict[idx] = image\n","\n","    if masked.nelement() != 0 and inverse_masked.nelement() != 0:\n","      if euclidian_masked <= euclidian_inverse_masked:\n","        query_masked_tensor_dict[idx] = masked\n","      else:\n","        query_masked_tensor_dict[idx] = inverse_masked\n","    else:\n","      query_masked_tensor_dict[idx] = torch.empty(0)\n","\n","    query_label_dict[idx] = label\n","    query_image_path_dict[idx] = image_path[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u7Mukg4HCQzI","executionInfo":{"status":"ok","timestamp":1698868978512,"user_tz":-330,"elapsed":2998,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"a6c9aa05-33f1-42db-e685-92205c30a494"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1935/1935 [00:02<00:00, 783.16it/s]\n"]}]},{"cell_type":"code","source":["MAX_VAL = 1000000\n","\n","def AP(actual_label, predicted_labels):\n","    GPTs = predicted_labels.count(actual_label)\n","    if GPTs < 1:\n","        return 1\n","    AP = 0\n","    GPTs_found = 0\n","    for i in range(len(predicted_labels)):\n","        if predicted_labels[i] == actual_label:\n","            GPTs_found += 1\n","            AP += GPTs_found / (i + 1)\n","    return AP / GPTs\n","\n","def mAP(APs):\n","    return np.average(APs)\n","\n","def get_similarity_score(q, qm, g, gm):\n","    if gm.nelement() != 0 and qm.nelement() != 0:\n","      distance = torch.nn.functional.pairwise_distance(qm, gm, p=2)\n","    else:\n","      distance = torch.nn.functional.pairwise_distance(q, g, p=2)\n","\n","    # distance = torch.nn.functional.pairwise_distance(q, g, p=2)\n","\n","    return torch.tensor(distance.item())\n","\n","\n","def get_mAP_of_model(query_tensor_dictionary, query_masked_tensor_dictionary, query_label_dictionary):\n","    APs_euclidean = []\n","    APs_manhattan = []\n","    APs_cosine = []\n","    for i in tqdm(range(len(query_tensor_dictionary))):\n","        query_tensor = query_tensor_dictionary[i].to('cuda')\n","        query_masked_tensor = query_masked_tensor_dictionary[i].to('cuda')\n","\n","        euclidean = None\n","        pred_labels = None\n","        act_label = query_label_dictionary[i]\n","\n","        for k in range(len(image_tensor_dict)):\n","          gallery_tensor = image_tensor_dict[k].to('cuda')\n","          gallery_masked_tensor = masked_tensor_dict[k].to('cuda')\n","          gallery_label = label_dict[k]\n","\n","          if euclidean == None:\n","            euclidean = torch.unsqueeze(get_similarity_score(query_tensor, query_masked_tensor, gallery_tensor, gallery_masked_tensor), 0)\n","            pred_labels = gallery_label\n","          else:\n","            distance = torch.unsqueeze(get_similarity_score(query_tensor, query_masked_tensor, gallery_tensor, gallery_masked_tensor), 0)\n","            euclidean = torch.cat((euclidean, distance), dim=0)\n","            pred_labels = torch.cat((pred_labels, gallery_label), dim=0)\n","\n","        euclidean_, euclidean_indices = torch.sort(euclidean)\n","\n","        pred_labels_euclidean = pred_labels[euclidean_indices]\n","\n","        APs_euclidean.append(AP(act_label.item(), pred_labels_euclidean.cpu().numpy().astype(int).tolist()))\n","\n","    print('\\nEuclidean distance mAP:', mAP(APs_euclidean))"],"metadata":{"id":"G2Ip3sPJCV8T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["get_mAP_of_model(query_image_tensor_dict, query_masked_tensor_dict, query_label_dict)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"saO0Eeo9Cilg","executionInfo":{"status":"ok","timestamp":1698958089703,"user_tz":-330,"elapsed":6,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"7b544446-ee6b-40be-942b-c688dbf20c29"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["100%|██████████| 1935/1935 [07:42<00:00,  4.18it/s]\n","Euclidean distance mAP: 0.6252792589434875\n"]}]},{"cell_type":"code","source":["del vit_model\n","torch.cuda.empty_cache()"],"metadata":{"id":"rpMhL5W9gkzA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"7XRPhFAvgpxd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698869773932,"user_tz":-330,"elapsed":536,"user":{"displayName":"Vimukthi Pahasara","userId":"10077165444938863537"}},"outputId":"72e4f85b-ff01-4b5d-fc47-d45185f55f6c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Nov  1 20:16:13 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   36C    P0    39W / 300W |   6238MiB / 16384MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}]}]}